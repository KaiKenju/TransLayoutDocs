{"type": "text", "bbox": [151, 1160, 817, 1496], "res": [{"text": "validation images come from the real scenes.", "confidence": 0.9807193279266357, "text_region": [[148.0, 1130.0], [654.0, 1130.0], [654.0, 1160.0], [148.0, 1160.0]]}, {"text": "For text recognition, there are 17.9M training images and", "confidence": 0.9814620614051819, "text_region": [[173.0, 1162.0], [815.0, 1162.0], [815.0, 1192.0], [173.0, 1192.0]]}, {"text": "18.7K validation images. Among the training images, 1.9M", "confidence": 0.9936009645462036, "text_region": [[150.0, 1192.0], [813.0, 1192.0], [813.0, 1224.0], [150.0, 1224.0]]}, {"text": "images are real scene images, which come from some pub-", "confidence": 0.9993777871131897, "text_region": [[148.0, 1224.0], [813.0, 1224.0], [813.0, 1254.0], [148.0, 1254.0]]}, {"text": "lic datasets and Baidu image search. The public datasets", "confidence": 0.990418553352356, "text_region": [[148.0, 1254.0], [815.0, 1254.0], [815.0, 1283.0], [148.0, 1283.0]]}, {"text": "used include LSVT, RCTW-17, MTWI 2018 and CCPD", "confidence": 0.9916078448295593, "text_region": [[148.0, 1283.0], [813.0, 1283.0], [813.0, 1313.0], [148.0, 1313.0]]}, {"text": "2019. The remaining 16M synthetic images mainly focus on", "confidence": 0.9997583627700806, "text_region": [[146.0, 1311.0], [815.0, 1313.0], [815.0, 1345.0], [145.0, 1343.0]]}, {"text": "the scenarios for different backgrounds, translation, rotation,", "confidence": 0.9946034550666809, "text_region": [[146.0, 1345.0], [813.0, 1345.0], [813.0, 1375.0], [146.0, 1375.0]]}, {"text": "perspective transformation, line disturb, noise, vertical text", "confidence": 0.9973934888839722, "text_region": [[148.0, 1375.0], [815.0, 1375.0], [815.0, 1405.0], [148.0, 1405.0]]}, {"text": "and so on. The corpus of synthetic images come from the", "confidence": 0.9943015575408936, "text_region": [[148.0, 1402.0], [813.0, 1402.0], [813.0, 1432.0], [148.0, 1432.0]]}, {"text": "real scene images. All the validation images also come from", "confidence": 0.9996377825737, "text_region": [[146.0, 1437.0], [815.0, 1437.0], [815.0, 1467.0], [146.0, 1467.0]]}, {"text": "the real scenes.", "confidence": 0.9994627237319946, "text_region": [[146.0, 1467.0], [319.0, 1469.0], [319.0, 1494.0], [145.0, 1492.0]]}, {"text": "In order to conduct ablation experiments quickly and", "confidence": 0.9897671341896057, "text_region": [[176.0, 1496.0], [813.0, 1496.0], [813.0, 1526.0], [176.0, 1526.0]]}], "img_idx": 0, "score": 0.9878129959106445}
{"type": "text", "bbox": [149, 885, 816, 1161], "res": [{"text": "For direction classification, there are 600k training images", "confidence": 0.9974585771560669, "text_region": [[176.0, 885.0], [815.0, 887.0], [815.0, 917.0], [175.0, 914.0]]}, {"text": "and 310K validation images. Among the training images,", "confidence": 0.9966419339179993, "text_region": [[146.0, 914.0], [815.0, 919.0], [815.0, 949.0], [145.0, 944.0]]}, {"text": "100K images are real scene images, which come from the", "confidence": 0.9988810420036316, "text_region": [[150.0, 949.0], [811.0, 949.0], [811.0, 979.0], [150.0, 979.0]]}, {"text": "public datasets (LSVT, RCTW-17, MTWI 2018). They are", "confidence": 0.9992185831069946, "text_region": [[150.0, 979.0], [813.0, 979.0], [813.0, 1008.0], [150.0, 1008.0]]}, {"text": "horizontal text which rectify and crop the ground truth of the", "confidence": 0.9860967397689819, "text_region": [[148.0, 1008.0], [813.0, 1008.0], [813.0, 1038.0], [148.0, 1038.0]]}, {"text": "images. The remaining 500K synthetic images mainly focus", "confidence": 0.9861977696418762, "text_region": [[148.0, 1038.0], [815.0, 1038.0], [815.0, 1070.0], [148.0, 1070.0]]}, {"text": "on the reversed text. We use the vertical fonts to synthesize", "confidence": 0.9945101737976074, "text_region": [[148.0, 1070.0], [815.0, 1070.0], [815.0, 1100.0], [148.0, 1100.0]]}, {"text": "some text images and then rotate them horizontally. All the", "confidence": 0.9998377561569214, "text_region": [[148.0, 1100.0], [815.0, 1100.0], [815.0, 1130.0], [148.0, 1130.0]]}, {"text": "validation images come from the real scenes.", "confidence": 0.9807193279266357, "text_region": [[148.0, 1130.0], [654.0, 1130.0], [654.0, 1160.0], [148.0, 1160.0]]}], "img_idx": 0, "score": 0.9862377047538757}
{"type": "text", "bbox": [886, 885, 1557, 1160], "res": [{"text": "synthesize the text line images for text recognition. Some", "confidence": 0.9970035552978516, "text_region": [[885.0, 887.0], [1552.0, 887.0], [1552.0, 917.0], [885.0, 917.0]]}, {"text": "images for alphanumeric symbols recognition come from", "confidence": 0.99592524766922, "text_region": [[885.0, 917.0], [1552.0, 917.0], [1552.0, 946.0], [885.0, 946.0]]}, {"text": "the public datasets, ST (Gupta, Vedaldi, and Zisserman", "confidence": 0.9838247895240784, "text_region": [[885.0, 949.0], [1554.0, 949.0], [1554.0, 979.0], [885.0, 979.0]]}, {"text": "2016) and SRN (Yu et al. 2020). Table2shows the statistics.", "confidence": 0.9836106300354004, "text_region": [[882.0, 979.0], [1552.0, 979.0], [1552.0, 1008.0], [882.0, 1008.0]]}, {"text": "Since MLT 2019 for text detection includes multilingual im-", "confidence": 0.9944859743118286, "text_region": [[885.0, 1004.0], [1552.0, 1006.0], [1552.0, 1038.0], [885.0, 1036.0]]}, {"text": "ages, the text detector for Chinese and English recognition", "confidence": 0.9944952726364136, "text_region": [[882.0, 1040.0], [1554.0, 1038.0], [1555.0, 1068.0], [882.0, 1070.0]]}, {"text": "also can support multi language text detection. Due to the", "confidence": 0.9871636033058167, "text_region": [[885.0, 1070.0], [1552.0, 1070.0], [1552.0, 1100.0], [885.0, 1100.0]]}, {"text": "limited data, we haven't found the proper data to train the", "confidence": 0.9773446917533875, "text_region": [[885.0, 1100.0], [1552.0, 1100.0], [1552.0, 1130.0], [885.0, 1130.0]]}, {"text": "direction classifier for multilingual.", "confidence": 0.9951983094215393, "text_region": [[885.0, 1130.0], [1282.0, 1130.0], [1282.0, 1160.0], [885.0, 1160.0]]}], "img_idx": 0, "score": 0.9859600067138672}
{"type": "text", "bbox": [887, 1516, 1555, 1822], "res": [{"text": "In the inference period, HMean is used to evaluate the per-", "confidence": 0.9850541949272156, "text_region": [[912.0, 1519.0], [1552.0, 1519.0], [1552.0, 1549.0], [912.0, 1549.0]]}, {"text": "formance of a text detector. Accuracy is used to evaluate the", "confidence": 0.9930621981620789, "text_region": [[887.0, 1547.0], [1552.0, 1547.0], [1552.0, 1577.0], [887.0, 1577.0]]}, {"text": "performance of a direction classifier or a text recognizer. F-", "confidence": 0.9997655749320984, "text_region": [[889.0, 1584.0], [1550.0, 1584.0], [1550.0, 1606.0], [889.0, 1606.0]]}, {"text": "score is used to evaluate the performance of an OCR system.", "confidence": 0.9985466003417969, "text_region": [[887.0, 1609.0], [1550.0, 1609.0], [1550.0, 1639.0], [887.0, 1639.0]]}, {"text": "In order to calculate F-score, a correct text recognition result", "confidence": 0.9933611154556274, "text_region": [[887.0, 1639.0], [1552.0, 1639.0], [1552.0, 1668.0], [887.0, 1668.0]]}, {"text": "should be the accurate location and the same text. GPU in-", "confidence": 0.9854326844215393, "text_region": [[887.0, 1673.0], [1545.0, 1673.0], [1545.0, 1696.0], [887.0, 1696.0]]}, {"text": "ference time is tested on a single T4 GPU. CPU inference", "confidence": 0.9980748295783997, "text_region": [[887.0, 1700.0], [1552.0, 1700.0], [1552.0, 1730.0], [887.0, 1730.0]]}, {"text": "time is tested on a Intel(R) Xeon(R) Gold 6148. We use the", "confidence": 0.9861988425254822, "text_region": [[885.0, 1732.0], [1552.0, 1732.0], [1552.0, 1762.0], [885.0, 1762.0]]}, {"text": "Snapdragon 855 (SD 855) to evaluate the inference time of", "confidence": 0.9990083575248718, "text_region": [[885.0, 1762.0], [1554.0, 1762.0], [1554.0, 1792.0], [885.0, 1792.0]]}, {"text": "the quantification models.", "confidence": 0.9997767210006714, "text_region": [[887.0, 1797.0], [1173.0, 1797.0], [1173.0, 1820.0], [887.0, 1820.0]]}], "img_idx": 0, "score": 0.9855121374130249}
{"type": "text", "bbox": [887, 1241, 1555, 1517], "res": [{"text": "Implementation Details We use Adam optimizer to train", "confidence": 0.9816985130310059, "text_region": [[887.0, 1247.0], [1550.0, 1247.0], [1550.0, 1270.0], [887.0, 1270.0]]}, {"text": "all the models and adopt cosine learning rate decay as the", "confidence": 0.9998970627784729, "text_region": [[887.0, 1274.0], [1552.0, 1274.0], [1552.0, 1304.0], [887.0, 1304.0]]}, {"text": "learning rate schedule. The initial learning rate, batch size", "confidence": 0.9892133474349976, "text_region": [[887.0, 1302.0], [1552.0, 1302.0], [1552.0, 1331.0], [887.0, 1331.0]]}, {"text": "and the number of epochs for different tasks can be found in", "confidence": 0.9981974959373474, "text_region": [[887.0, 1334.0], [1554.0, 1334.0], [1554.0, 1364.0], [887.0, 1364.0]]}, {"text": "Table When we obtain the trained models, FPGM pruner", "confidence": 0.9849205613136292, "text_region": [[887.0, 1366.0], [1552.0, 1366.0], [1552.0, 1396.0], [887.0, 1396.0]]}, {"text": "and PACT quantization can be used to reduce the model size", "confidence": 0.9909151196479797, "text_region": [[889.0, 1398.0], [1550.0, 1398.0], [1550.0, 1421.0], [889.0, 1421.0]]}, {"text": "further with the above models as the pre-trained ones. The", "confidence": 0.9934219717979431, "text_region": [[885.0, 1423.0], [1552.0, 1425.0], [1552.0, 1455.0], [885.0, 1453.0]]}, {"text": "training processes of FPGM pruner and PACT quantization", "confidence": 0.9914555549621582, "text_region": [[885.0, 1455.0], [1552.0, 1455.0], [1552.0, 1485.0], [885.0, 1485.0]]}, {"text": "are similar as previous.", "confidence": 0.9996428489685059, "text_region": [[885.0, 1485.0], [1143.0, 1485.0], [1143.0, 1515.0], [885.0, 1515.0]]}], "img_idx": 0, "score": 0.9848317503929138}
{"type": "text", "bbox": [149, 1650, 817, 1834], "res": [{"text": "In addition, we collected 300 images for different real ap.", "confidence": 0.996408224105835, "text_region": [[176.0, 1650.0], [806.0, 1650.0], [806.0, 1680.0], [176.0, 1680.0]]}, {"text": "plication scenarios to evaluate the overall OCR system, in-", "confidence": 0.9964815974235535, "text_region": [[148.0, 1682.0], [811.0, 1682.0], [811.0, 1712.0], [148.0, 1712.0]]}, {"text": "cluding contract samples, license plates, nameplates, train", "confidence": 0.9963365197181702, "text_region": [[148.0, 1714.0], [815.0, 1714.0], [815.0, 1744.0], [148.0, 1744.0]]}, {"text": "tickets, test sheets, forms, certificates, street view images,", "confidence": 0.97979336977005, "text_region": [[148.0, 1744.0], [813.0, 1744.0], [813.0, 1774.0], [148.0, 1774.0]]}, {"text": "business cards, digital meter, etc. Figure[3and Figure4show", "confidence": 0.9868564009666443, "text_region": [[148.0, 1774.0], [813.0, 1774.0], [813.0, 1804.0], [148.0, 1804.0]]}, {"text": "some images of the test set.", "confidence": 0.9911640286445618, "text_region": [[148.0, 1808.0], [460.0, 1808.0], [460.0, 1831.0], [148.0, 1831.0]]}], "img_idx": 0, "score": 0.9764607548713684}
{"type": "text", "bbox": [150, 1496, 818, 1648], "res": [{"text": "In order to conduct ablation experiments quickly and", "confidence": 0.9897671341896057, "text_region": [[176.0, 1496.0], [813.0, 1496.0], [813.0, 1526.0], [176.0, 1526.0]]}, {"text": "choose the appropriate strategies, we select 4k images from", "confidence": 0.9981034994125366, "text_region": [[146.0, 1526.0], [811.0, 1529.0], [811.0, 1558.0], [145.0, 1556.0]]}, {"text": "the real scene training images for text detection, and 300k", "confidence": 0.9926935434341431, "text_region": [[148.0, 1561.0], [813.0, 1561.0], [813.0, 1590.0], [148.0, 1590.0]]}, {"text": "ones from the real scene training images for text recogni-", "confidence": 0.9899305105209351, "text_region": [[146.0, 1590.0], [813.0, 1590.0], [813.0, 1620.0], [146.0, 1620.0]]}, {"text": "tion.", "confidence": 0.9999316930770874, "text_region": [[148.0, 1622.0], [203.0, 1622.0], [203.0, 1648.0], [148.0, 1648.0]]}], "img_idx": 0, "score": 0.9755363464355469}
{"type": "text", "bbox": [150, 1833, 818, 1957], "res": [{"text": "Furthermore, to verify the proposed PP-OCR for other", "confidence": 0.9917513132095337, "text_region": [[173.0, 1836.0], [815.0, 1836.0], [815.0, 1865.0], [173.0, 1865.0]]}, {"text": "languages, we also collect some corpus for alphanumeric", "confidence": 0.9963595867156982, "text_region": [[148.0, 1870.0], [811.0, 1870.0], [811.0, 1893.0], [148.0, 1893.0]]}, {"text": "symbols recognition, French recognition, Korean recogni-", "confidence": 0.9999237060546875, "text_region": [[148.0, 1898.0], [813.0, 1898.0], [813.0, 1927.0], [148.0, 1927.0]]}, {"text": "tion, Japanese recognition and German recognition. Then", "confidence": 0.9996413588523865, "text_region": [[148.0, 1932.0], [813.0, 1932.0], [813.0, 1955.0], [148.0, 1955.0]]}], "img_idx": 0, "score": 0.9659725427627563}
{"type": "text", "bbox": [888, 1159, 1555, 1223], "res": [{"text": "direction classifier for multilingual.", "confidence": 0.9951983094215393, "text_region": [[885.0, 1130.0], [1282.0, 1130.0], [1282.0, 1160.0], [885.0, 1160.0]]}, {"text": "The data synthesis tool used in text detection and text", "confidence": 0.9876573085784912, "text_region": [[915.0, 1162.0], [1552.0, 1162.0], [1552.0, 1192.0], [915.0, 1192.0]]}, {"text": "recognition is modified from text render (Sanster[2018).", "confidence": 0.9747081995010376, "text_region": [[882.0, 1192.0], [1506.0, 1189.0], [1506.0, 1221.0], [882.0, 1224.0]]}], "img_idx": 0, "score": 0.9496194124221802}
{"type": "text", "bbox": [533, 794, 1139, 825], "res": [{"text": "Table 2: Statistics of dataset for multilingual recognition.", "confidence": 0.9900223612785339, "text_region": [[534.0, 795.0], [1164.0, 795.0], [1164.0, 825.0], [534.0, 825.0]]}], "img_idx": 0, "score": 0.8921920657157898}
{"type": "text", "bbox": [918, 1893, 1542, 1957], "res": [{"text": "Table 5] compares the performance of the different back-", "confidence": 0.9924523234367371, "text_region": [[885.0, 1893.0], [1548.0, 1895.0], [1547.0, 1927.0], [885.0, 1925.0]]}, {"text": "bones for text detection. HMean, the model size and the in-", "confidence": 0.9994714260101318, "text_region": [[882.0, 1927.0], [1550.0, 1927.0], [1550.0, 1957.0], [882.0, 1957.0]]}], "img_idx": 0, "score": 0.8639272451400757}
{"type": "text", "bbox": [560, 344, 1184, 376], "res": [{"text": "Table 1: Statistics of dataset for Chinese and English Recognition.", "confidence": 0.9971246123313904, "text_region": [[478.0, 341.0], [1215.0, 344.0], [1215.0, 376.0], [478.0, 374.0]]}], "img_idx": 0, "score": 0.7637348771095276}
{"type": "title", "bbox": [887, 1848, 1142, 1885], "res": [{"text": "3.2Text Detection", "confidence": 0.9940789341926575, "text_region": [[883.0, 1849.0], [1146.0, 1852.0], [1145.0, 1884.0], [882.0, 1881.0]]}], "img_idx": 0, "score": 0.8678309321403503}
{"type": "figure", "bbox": [283, 405, 1412, 755], "res": [{"text": " Number of training data", "confidence": 0.9815241694450378, "text_region": [[755.0, 406.0], [1033.0, 408.0], [1032.0, 440.0], [755.0, 438.0]]}, {"text": " Number of validation data", "confidence": 0.981758713722229, "text_region": [[1086.0, 406.0], [1391.0, 408.0], [1390.0, 440.0], [1086.0, 438.0]]}, {"text": "Character", "confidence": 0.999862551689148, "text_region": [[597.0, 435.0], [710.0, 440.0], [708.0, 473.0], [595.0, 467.0]]}, {"text": "Task", "confidence": 0.9998860955238342, "text_region": [[404.0, 454.0], [467.0, 454.0], [467.0, 488.0], [404.0, 488.0]]}, {"text": "Total", "confidence": 0.9999248385429382, "text_region": [[748.0, 454.0], [813.0, 454.0], [813.0, 488.0], [748.0, 488.0]]}, {"text": "Real", "confidence": 0.9995671510696411, "text_region": [[850.0, 448.0], [913.0, 455.0], [910.0, 489.0], [847.0, 482.0]]}, {"text": "Synthesis", "confidence": 0.9998038411140442, "text_region": [[938.0, 456.0], [1056.0, 456.0], [1056.0, 488.0], [938.0, 488.0]]}, {"text": "Total", "confidence": 0.9999176263809204, "text_region": [[1086.0, 454.0], [1153.0, 454.0], [1153.0, 488.0], [1086.0, 488.0]]}, {"text": "Real", "confidence": 0.9996513724327087, "text_region": [[1192.0, 454.0], [1254.0, 454.0], [1254.0, 488.0], [1192.0, 488.0]]}, {"text": "Synthesis", "confidence": 0.9998548030853271, "text_region": [[1284.0, 456.0], [1400.0, 456.0], [1400.0, 488.0], [1284.0, 488.0]]}, {"text": "Number", "confidence": 0.9998177886009216, "text_region": [[603.0, 470.0], [704.0, 470.0], [704.0, 502.0], [603.0, 502.0]]}, {"text": "Chinese and English", "confidence": 0.9998554587364197, "text_region": [[319.0, 502.0], [552.0, 502.0], [552.0, 534.0], [319.0, 534.0]]}, {"text": "6622", "confidence": 0.9998950958251953, "text_region": [[621.0, 516.0], [688.0, 516.0], [688.0, 550.0], [621.0, 550.0]]}, {"text": "17.9M", "confidence": 0.9803205728530884, "text_region": [[739.0, 516.0], [820.0, 516.0], [820.0, 548.0], [739.0, 548.0]]}, {"text": "1.9M", "confidence": 0.9970232248306274, "text_region": [[848.0, 520.0], [912.0, 520.0], [912.0, 550.0], [848.0, 550.0]]}, {"text": "16M", "confidence": 0.9986414909362793, "text_region": [[965.0, 516.0], [1028.0, 516.0], [1028.0, 550.0], [965.0, 550.0]]}, {"text": "18.7K", "confidence": 0.9999345541000366, "text_region": [[1081.0, 513.0], [1156.0, 519.0], [1153.0, 553.0], [1078.0, 547.0]]}, {"text": "18.7K", "confidence": 0.9999596476554871, "text_region": [[1185.0, 518.0], [1261.0, 518.0], [1261.0, 550.0], [1185.0, 550.0]]}, {"text": "0", "confidence": 0.9892804026603699, "text_region": [[1326.0, 518.0], [1356.0, 518.0], [1356.0, 550.0], [1326.0, 550.0]]}, {"text": "Recognition", "confidence": 0.9897755980491638, "text_region": [[363.0, 529.0], [506.0, 534.0], [505.0, 566.0], [362.0, 561.0]]}, {"text": " Alphanumeric Symbols", "confidence": 0.9857821464538574, "text_region": [[296.0, 566.0], [568.0, 566.0], [568.0, 596.0], [296.0, 596.0]]}, {"text": "63", "confidence": 0.9998565912246704, "text_region": [[635.0, 580.0], [672.0, 580.0], [672.0, 612.0], [635.0, 612.0]]}, {"text": "15M", "confidence": 0.9927738308906555, "text_region": [[751.0, 582.0], [808.0, 582.0], [808.0, 610.0], [751.0, 610.0]]}, {"text": "0", "confidence": 0.9956016540527344, "text_region": [[868.0, 582.0], [889.0, 582.0], [889.0, 607.0], [868.0, 607.0]]}, {"text": "15M", "confidence": 0.9920849800109863, "text_region": [[968.0, 582.0], [1026.0, 582.0], [1026.0, 610.0], [968.0, 610.0]]}, {"text": "12K", "confidence": 0.999943733215332, "text_region": [[1093.0, 582.0], [1146.0, 582.0], [1146.0, 610.0], [1093.0, 610.0]]}, {"text": "12K", "confidence": 0.9998881220817566, "text_region": [[1194.0, 578.0], [1250.0, 578.0], [1250.0, 612.0], [1194.0, 612.0]]}, {"text": "0", "confidence": 0.9855157732963562, "text_region": [[1326.0, 580.0], [1356.0, 580.0], [1356.0, 610.0], [1326.0, 610.0]]}, {"text": "Recognition", "confidence": 0.9998801946640015, "text_region": [[365.0, 596.0], [501.0, 596.0], [501.0, 621.0], [365.0, 621.0]]}, {"text": "French Recognition", "confidence": 0.9998810291290283, "text_region": [[323.0, 628.0], [547.0, 628.0], [547.0, 658.0], [323.0, 658.0]]}, {"text": "118", "confidence": 0.9998459219932556, "text_region": [[633.0, 628.0], [677.0, 628.0], [677.0, 655.0], [633.0, 655.0]]}, {"text": "1.08M", "confidence": 0.9984372854232788, "text_region": [[736.0, 620.0], [818.0, 626.0], [816.0, 658.0], [734.0, 652.0]]}, {"text": "1.08M", "confidence": 0.9977437853813171, "text_region": [[960.0, 623.0], [1033.0, 629.0], [1031.0, 656.0], [958.0, 650.0]]}, {"text": "80K", "confidence": 0.995002269744873, "text_region": [[1093.0, 628.0], [1143.0, 628.0], [1143.0, 655.0], [1093.0, 655.0]]}, {"text": "80K", "confidence": 0.99507737159729, "text_region": [[1312.0, 620.0], [1369.0, 627.0], [1365.0, 659.0], [1308.0, 652.0]]}, {"text": "0", "confidence": 0.8841418623924255, "text_region": [[871.0, 635.0], [887.0, 635.0], [887.0, 651.0], [871.0, 651.0]]}, {"text": "0", "confidence": 0.9965260624885559, "text_region": [[1210.0, 630.0], [1231.0, 630.0], [1231.0, 653.0], [1210.0, 653.0]]}, {"text": "Japanese Recognition", "confidence": 0.9866629838943481, "text_region": [[312.0, 658.0], [559.0, 658.0], [559.0, 688.0], [312.0, 688.0]]}, {"text": "4399", "confidence": 0.9999591112136841, "text_region": [[626.0, 658.0], [684.0, 658.0], [684.0, 685.0], [626.0, 685.0]]}, {"text": "0.99M", "confidence": 0.9967422485351562, "text_region": [[737.0, 655.0], [820.0, 655.0], [820.0, 688.0], [737.0, 688.0]]}, {"text": "0.99M", "confidence": 0.9431165456771851, "text_region": [[956.0, 655.0], [1037.0, 655.0], [1037.0, 688.0], [956.0, 688.0]]}, {"text": "80K", "confidence": 0.9969606399536133, "text_region": [[1093.0, 650.0], [1144.0, 656.0], [1140.0, 688.0], [1089.0, 682.0]]}, {"text": "0", "confidence": 0.9958968162536621, "text_region": [[868.0, 660.0], [889.0, 660.0], [889.0, 683.0], [868.0, 683.0]]}, {"text": "0", "confidence": 0.9953371286392212, "text_region": [[1210.0, 660.0], [1231.0, 660.0], [1231.0, 683.0], [1210.0, 683.0]]}, {"text": "80K", "confidence": 0.9984674453735352, "text_region": [[1312.0, 655.0], [1370.0, 655.0], [1370.0, 690.0], [1312.0, 690.0]]}, {"text": "Korean Recognition", "confidence": 0.9999535083770752, "text_region": [[321.0, 690.0], [552.0, 690.0], [552.0, 720.0], [321.0, 720.0]]}, {"text": "3636", "confidence": 0.9999408721923828, "text_region": [[626.0, 690.0], [686.0, 690.0], [686.0, 717.0], [626.0, 717.0]]}, {"text": "0.94M", "confidence": 0.9904811978340149, "text_region": [[737.0, 692.0], [818.0, 692.0], [818.0, 717.0], [737.0, 717.0]]}, {"text": "0", "confidence": 0.9979013204574585, "text_region": [[868.0, 694.0], [889.0, 694.0], [889.0, 717.0], [868.0, 717.0]]}, {"text": "0.94M", "confidence": 0.9951836466789246, "text_region": [[959.0, 692.0], [1035.0, 692.0], [1035.0, 717.0], [959.0, 717.0]]}, {"text": "80K", "confidence": 0.9987072944641113, "text_region": [[1090.0, 690.0], [1143.0, 690.0], [1143.0, 717.0], [1090.0, 717.0]]}, {"text": "0", "confidence": 0.9970101118087769, "text_region": [[1210.0, 692.0], [1231.0, 692.0], [1231.0, 717.0], [1210.0, 717.0]]}, {"text": "80K", "confidence": 0.9973545074462891, "text_region": [[1314.0, 690.0], [1367.0, 690.0], [1367.0, 717.0], [1314.0, 717.0]]}, {"text": "German Recognition", "confidence": 0.99993896484375, "text_region": [[317.0, 719.0], [552.0, 722.0], [552.0, 752.0], [316.0, 749.0]]}, {"text": "131", "confidence": 0.9999629855155945, "text_region": [[631.0, 722.0], [679.0, 722.0], [679.0, 754.0], [631.0, 754.0]]}, {"text": "1.96M", "confidence": 0.997270941734314, "text_region": [[737.0, 720.0], [820.0, 720.0], [820.0, 752.0], [737.0, 752.0]]}, {"text": "0", "confidence": 0.9959814548492432, "text_region": [[868.0, 726.0], [889.0, 726.0], [889.0, 747.0], [868.0, 747.0]]}, {"text": "1.96M", "confidence": 0.9986785054206848, "text_region": [[956.0, 720.0], [1037.0, 720.0], [1037.0, 752.0], [956.0, 752.0]]}, {"text": "170K", "confidence": 0.9994982481002808, "text_region": [[1088.0, 722.0], [1153.0, 722.0], [1153.0, 752.0], [1088.0, 752.0]]}, {"text": "0", "confidence": 0.9972966313362122, "text_region": [[1210.0, 726.0], [1231.0, 726.0], [1231.0, 747.0], [1210.0, 747.0]]}, {"text": "170K", "confidence": 0.9995433688163757, "text_region": [[1312.0, 722.0], [1377.0, 722.0], [1377.0, 752.0], [1312.0, 752.0]]}], "img_idx": 0, "score": 0.9369564652442932}
{"type": "figure", "bbox": [375, 147, 1329, 307], "res": [{"text": "Number of training data", "confidence": 0.9929813146591187, "text_region": [[698.0, 151.0], [972.0, 151.0], [972.0, 181.0], [698.0, 181.0]]}, {"text": " Number of validation data", "confidence": 0.9845498204231262, "text_region": [[1016.0, 147.0], [1326.0, 147.0], [1326.0, 176.0], [1016.0, 176.0]]}, {"text": "Task", "confidence": 0.999832034111023, "text_region": [[483.0, 176.0], [547.0, 176.0], [547.0, 211.0], [483.0, 211.0]]}, {"text": "Total", "confidence": 0.9999569058418274, "text_region": [[688.0, 181.0], [748.0, 181.0], [748.0, 209.0], [688.0, 209.0]]}, {"text": "Real", "confidence": 0.999772846698761, "text_region": [[790.0, 183.0], [848.0, 183.0], [848.0, 209.0], [790.0, 209.0]]}, {"text": "Synthesis", "confidence": 0.9999269843101501, "text_region": [[878.0, 183.0], [993.0, 183.0], [993.0, 209.0], [878.0, 209.0]]}, {"text": "Real", "confidence": 0.9998543858528137, "text_region": [[1143.0, 181.0], [1199.0, 181.0], [1199.0, 209.0], [1143.0, 209.0]]}, {"text": "Text Detection", "confidence": 0.9999106526374817, "text_region": [[434.0, 213.0], [598.0, 213.0], [598.0, 238.0], [434.0, 238.0]]}, {"text": "97K", "confidence": 0.9999217987060547, "text_region": [[686.0, 209.0], [746.0, 209.0], [746.0, 243.0], [686.0, 243.0]]}, {"text": "68K", "confidence": 0.9999508857727051, "text_region": [[792.0, 213.0], [845.0, 213.0], [845.0, 241.0], [792.0, 241.0]]}, {"text": "29K", "confidence": 0.9999457001686096, "text_region": [[910.0, 213.0], [963.0, 213.0], [963.0, 241.0], [910.0, 241.0]]}, {"text": " 500", "confidence": 0.9180103540420532, "text_region": [[1141.0, 209.0], [1199.0, 209.0], [1199.0, 243.0], [1141.0, 243.0]]}, {"text": "Direction Classification", "confidence": 0.9999290108680725, "text_region": [[379.0, 238.0], [649.0, 241.0], [649.0, 273.0], [379.0, 270.0]]}, {"text": "600K", "confidence": 0.9989275336265564, "text_region": [[683.0, 235.0], [754.0, 242.0], [751.0, 274.0], [680.0, 267.0]]}, {"text": "100K", "confidence": 0.9998546838760376, "text_region": [[785.0, 235.0], [853.0, 242.0], [850.0, 274.0], [782.0, 267.0]]}, {"text": " 500K", "confidence": 0.9257143139839172, "text_region": [[901.0, 235.0], [973.0, 242.0], [970.0, 276.0], [898.0, 270.0]]}, {"text": "310K", "confidence": 0.9996426701545715, "text_region": [[1134.0, 235.0], [1207.0, 242.0], [1204.0, 274.0], [1131.0, 267.0]]}, {"text": "Text Recognition", "confidence": 0.953835666179657, "text_region": [[413.0, 273.0], [614.0, 273.0], [614.0, 302.0], [413.0, 302.0]]}, {"text": "17.9M", "confidence": 0.9974380731582642, "text_region": [[679.0, 270.0], [760.0, 270.0], [760.0, 302.0], [679.0, 302.0]]}, {"text": "1.9M", "confidence": 0.9434299468994141, "text_region": [[785.0, 270.0], [857.0, 270.0], [857.0, 302.0], [785.0, 302.0]]}, {"text": "16M", "confidence": 0.9984113574028015, "text_region": [[908.0, 273.0], [968.0, 273.0], [968.0, 300.0], [908.0, 300.0]]}, {"text": "18.7K", "confidence": 0.9999569654464722, "text_region": [[1134.0, 270.0], [1208.0, 270.0], [1208.0, 302.0], [1134.0, 302.0]]}], "img_idx": 0, "score": 0.9071248769760132}
