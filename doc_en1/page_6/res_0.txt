{"type": "text", "bbox": [887, 154, 1555, 489], "res": [{"text": "the model performs well on the training data, but it performs", "confidence": 0.997713565826416, "text_region": [[885.0, 154.0], [1550.0, 154.0], [1550.0, 183.0], [885.0, 183.0]]}, {"text": "poorly on the test data. To avoid overfitting, many regular", "confidence": 0.9912261962890625, "text_region": [[887.0, 186.0], [1552.0, 186.0], [1552.0, 215.0], [887.0, 215.0]]}, {"text": "ways have been proposed. Among them, weight_decay is", "confidence": 0.9903101325035095, "text_region": [[889.0, 220.0], [1550.0, 220.0], [1550.0, 243.0], [889.0, 243.0]]}, {"text": "one of the widely used ways to avoid overfitting. After the", "confidence": 0.9938111305236816, "text_region": [[887.0, 245.0], [1554.0, 245.0], [1554.0, 275.0], [887.0, 275.0]]}, {"text": "final loss function, L2 regularization (L2_decay) is added to", "confidence": 0.9856376051902771, "text_region": [[887.0, 277.0], [1552.0, 277.0], [1552.0, 307.0], [887.0, 307.0]]}, {"text": "the loss function. With the help of L2 regularization, the", "confidence": 0.9928058385848999, "text_region": [[887.0, 307.0], [1552.0, 307.0], [1552.0, 337.0], [887.0, 337.0]]}, {"text": "weight of the network tend to choose a smaller value, and", "confidence": 0.9959328770637512, "text_region": [[889.0, 341.0], [1550.0, 341.0], [1550.0, 364.0], [889.0, 364.0]]}, {"text": "finally the parameters in the entire network tends to O, and", "confidence": 0.9913319945335388, "text_region": [[889.0, 371.0], [1550.0, 371.0], [1550.0, 394.0], [889.0, 394.0]]}, {"text": "the generalization performance of the model is improved ac-", "confidence": 0.9839136600494385, "text_region": [[887.0, 399.0], [1548.0, 399.0], [1548.0, 429.0], [887.0, 429.0]]}, {"text": "cordingly. For text recognition, L2_decay has a great influ-", "confidence": 0.9935226440429688, "text_region": [[887.0, 429.0], [1548.0, 429.0], [1548.0, 458.0], [887.0, 458.0]]}, {"text": "ence on the accuracy.", "confidence": 0.9698877930641174, "text_region": [[887.0, 458.0], [1123.0, 461.0], [1122.0, 491.0], [887.0, 488.0]]}], "img_idx": 0, "score": 0.9886720776557922}
{"type": "text", "bbox": [150, 1463, 818, 1892], "res": [{"text": "Feature Map Resolution In order to adapt to multilin-", "confidence": 0.9785245656967163, "text_region": [[176.0, 1467.0], [811.0, 1467.0], [811.0, 1496.0], [176.0, 1496.0]]}, {"text": "gual recognition, particularly in Chinese recognition, in PP-", "confidence": 0.9924967885017395, "text_region": [[148.0, 1496.0], [811.0, 1496.0], [811.0, 1526.0], [148.0, 1526.0]]}, {"text": "OCR the height and width of the CRNN input are set as 32", "confidence": 0.9902653098106384, "text_region": [[150.0, 1531.0], [813.0, 1531.0], [813.0, 1554.0], [150.0, 1554.0]]}, {"text": "and 320. Then, the strides of the original MobileNetV3 is", "confidence": 0.9948606491088867, "text_region": [[148.0, 1558.0], [813.0, 1558.0], [813.0, 1588.0], [148.0, 1588.0]]}, {"text": "not appropriate for text recognition. As shown in Figure[12", "confidence": 0.9935187101364136, "text_region": [[146.0, 1586.0], [811.0, 1588.0], [811.0, 1620.0], [145.0, 1618.0]]}, {"text": "for the sake of keeping more the horizontal information, we", "confidence": 0.9979947209358215, "text_region": [[148.0, 1618.0], [813.0, 1618.0], [813.0, 1648.0], [148.0, 1648.0]]}, {"text": "modify the stride of the down sampling feature map except", "confidence": 0.9913557171821594, "text_region": [[148.0, 1652.0], [813.0, 1652.0], [813.0, 1675.0], [148.0, 1675.0]]}, {"text": "the first one from (2,2) to (2,1). In order to keep more verti.", "confidence": 0.9870562553405762, "text_region": [[148.0, 1680.0], [808.0, 1680.0], [808.0, 1710.0], [148.0, 1710.0]]}, {"text": "cal information, we further modify the stride of the second", "confidence": 0.999869704246521, "text_region": [[150.0, 1714.0], [813.0, 1714.0], [813.0, 1737.0], [150.0, 1737.0]]}, {"text": "down sampling feature map from (2,1) to (1,1). Thus, the", "confidence": 0.9890829920768738, "text_region": [[148.0, 1744.0], [813.0, 1744.0], [813.0, 1767.0], [148.0, 1767.0]]}, {"text": "stride of the second down sampling feature map s2 affects", "confidence": 0.9889547228813171, "text_region": [[148.0, 1771.0], [813.0, 1771.0], [813.0, 1801.0], [148.0, 1801.0]]}, {"text": "the resolution of the whole feature map and the accuracy of", "confidence": 0.9921002388000488, "text_region": [[150.0, 1806.0], [813.0, 1806.0], [813.0, 1829.0], [150.0, 1829.0]]}, {"text": "the text recognizer dramaticly. In PP-OCR, s2 is set as (1,1)", "confidence": 0.9967700839042664, "text_region": [[143.0, 1829.0], [815.0, 1831.0], [815.0, 1863.0], [143.0, 1861.0]]}, {"text": "to achieve the better performance empirically.", "confidence": 0.9886837005615234, "text_region": [[146.0, 1863.0], [661.0, 1863.0], [661.0, 1893.0], [146.0, 1893.0]]}], "img_idx": 0, "score": 0.9883482456207275}
{"type": "text", "bbox": [887, 885, 1555, 1192], "res": [{"text": "set to 48 empirically.", "confidence": 0.9977990984916687, "text_region": [[885.0, 857.0], [1125.0, 857.0], [1125.0, 887.0], [885.0, 887.0]]}, {"text": "Pre-trained Model If the training data is fewer, fine tune", "confidence": 0.985429584980011, "text_region": [[912.0, 889.0], [1552.0, 889.0], [1552.0, 919.0], [912.0, 919.0]]}, {"text": "the existing networks, which are trained on a large data set", "confidence": 0.9946127533912659, "text_region": [[885.0, 919.0], [1552.0, 919.0], [1552.0, 949.0], [885.0, 949.0]]}, {"text": "such as ImageNet, to achieve fast convergence and better", "confidence": 0.9918639063835144, "text_region": [[889.0, 953.0], [1550.0, 953.0], [1550.0, 976.0], [889.0, 976.0]]}, {"text": "accuracy. The transfer learning in image classification and", "confidence": 0.982635498046875, "text_region": [[887.0, 981.0], [1552.0, 981.0], [1552.0, 1011.0], [887.0, 1011.0]]}, {"text": "object detection show the above strategy is effective. In real", "confidence": 0.995101809501648, "text_region": [[887.0, 1011.0], [1552.0, 1011.0], [1552.0, 1040.0], [887.0, 1040.0]]}, {"text": "scenes, the data used for text recognition is often limited. If", "confidence": 0.9967942237854004, "text_region": [[887.0, 1040.0], [1552.0, 1040.0], [1552.0, 1070.0], [887.0, 1070.0]]}, {"text": "the models are trained with tens of millions samples, even if", "confidence": 0.9984155297279358, "text_region": [[885.0, 1072.0], [1552.0, 1072.0], [1552.0, 1102.0], [885.0, 1102.0]]}, {"text": "they are synthesized ones, the accuracy can be significantly", "confidence": 0.990919291973114, "text_region": [[885.0, 1102.0], [1548.0, 1102.0], [1548.0, 1132.0], [885.0, 1132.0]]}, {"text": "improved with the above models. We demonstrate the effec-", "confidence": 0.9956546425819397, "text_region": [[887.0, 1132.0], [1548.0, 1132.0], [1548.0, 1162.0], [887.0, 1162.0]]}, {"text": "tiveness of this strategy through experiments.", "confidence": 0.9787181615829468, "text_region": [[887.0, 1166.0], [1388.0, 1166.0], [1388.0, 1189.0], [887.0, 1189.0]]}], "img_idx": 0, "score": 0.9841442108154297}
{"type": "text", "bbox": [887, 1559, 1555, 1957], "res": [{"text": "For text detection, there are 97k training images and 500", "confidence": 0.9997591972351074, "text_region": [[912.0, 1561.0], [1552.0, 1561.0], [1552.0, 1590.0], [912.0, 1590.0]]}, {"text": "validation images. Among the training images, 68K im-", "confidence": 0.9810853004455566, "text_region": [[885.0, 1590.0], [1548.0, 1590.0], [1548.0, 1620.0], [885.0, 1620.0]]}, {"text": "ages are real scene images, which come from some public", "confidence": 0.9913855791091919, "text_region": [[887.0, 1622.0], [1550.0, 1622.0], [1550.0, 1652.0], [887.0, 1652.0]]}, {"text": "datasets and Baidu image search. The public datasets used", "confidence": 0.999924898147583, "text_region": [[887.0, 1652.0], [1552.0, 1652.0], [1552.0, 1682.0], [887.0, 1682.0]]}, {"text": "include LSVT (Sun et al.[2019), RCTW-17 (Shi et al.[2017)", "confidence": 0.9720438718795776, "text_region": [[885.0, 1682.0], [1545.0, 1682.0], [1545.0, 1712.0], [885.0, 1712.0]]}, {"text": "MTWI 2018 (He and Yang|2018), CASIA-10K (He et al.", "confidence": 0.9876158833503723, "text_region": [[885.0, 1714.0], [1552.0, 1714.0], [1552.0, 1744.0], [885.0, 1744.0]]}, {"text": "2018), SROIE (Huang et al.2019), MLT 2019 (Nayef et al.", "confidence": 0.96735680103302, "text_region": [[885.0, 1744.0], [1552.0, 1744.0], [1552.0, 1774.0], [885.0, 1774.0]]}, {"text": "2019), BD1 (Karatzas et al.2011), MSRA-TD500 (Yao et al.", "confidence": 0.9787028431892395, "text_region": [[885.0, 1774.0], [1552.0, 1774.0], [1552.0, 1804.0], [885.0, 1804.0]]}, {"text": "2012) and CCPD 2019 (Xu et al.2018). Most the training", "confidence": 0.9826588034629822, "text_region": [[885.0, 1806.0], [1552.0, 1806.0], [1552.0, 1836.0], [885.0, 1836.0]]}, {"text": "images from Baidu image search are document text images.", "confidence": 0.9947346448898315, "text_region": [[885.0, 1836.0], [1548.0, 1836.0], [1548.0, 1865.0], [885.0, 1865.0]]}, {"text": "The remaining 29K synthetic images mainly focus on the", "confidence": 0.9917630553245544, "text_region": [[887.0, 1868.0], [1552.0, 1868.0], [1552.0, 1898.0], [887.0, 1898.0]]}, {"text": "scenarios for long text, multi direction text and table text.", "confidence": 0.9999430179595947, "text_region": [[887.0, 1898.0], [1552.0, 1898.0], [1552.0, 1927.0], [887.0, 1927.0]]}, {"text": "All the validation images come from the real scenes.", "confidence": 0.9998904466629028, "text_region": [[885.0, 1927.0], [1471.0, 1927.0], [1471.0, 1957.0], [885.0, 1957.0]]}], "img_idx": 0, "score": 0.9839733839035034}
{"type": "text", "bbox": [887, 611, 1556, 886], "res": [{"text": "strategy is also effective.", "confidence": 0.9808313846588135, "text_region": [[885.0, 582.0], [1162.0, 582.0], [1162.0, 612.0], [885.0, 612.0]]}, {"text": "Light Head A full connection layer is used to encode the", "confidence": 0.9864473342895508, "text_region": [[912.0, 614.0], [1552.0, 614.0], [1552.0, 644.0], [912.0, 644.0]]}, {"text": "sequence features to the predicted characters in the ordinary.", "confidence": 0.9772346615791321, "text_region": [[887.0, 644.0], [1548.0, 644.0], [1548.0, 674.0], [887.0, 674.0]]}, {"text": "The dimension of the sequence features have an impact on", "confidence": 0.9956855177879333, "text_region": [[887.0, 674.0], [1552.0, 674.0], [1552.0, 704.0], [887.0, 704.0]]}, {"text": "the model size of a text recognizer, especially for Chinese", "confidence": 0.9937183856964111, "text_region": [[885.0, 706.0], [1552.0, 706.0], [1552.0, 736.0], [885.0, 736.0]]}, {"text": "recognition whose characters are more than 6 thousands.", "confidence": 0.9981818199157715, "text_region": [[887.0, 736.0], [1548.0, 736.0], [1548.0, 765.0], [887.0, 765.0]]}, {"text": "Meanwhile, it is not that the higher of the dimension, the", "confidence": 0.99500572681427, "text_region": [[885.0, 765.0], [1552.0, 765.0], [1552.0, 795.0], [885.0, 795.0]]}, {"text": "stronger of the ability of the sequence features representa-", "confidence": 0.9807989597320557, "text_region": [[887.0, 798.0], [1550.0, 798.0], [1550.0, 827.0], [887.0, 827.0]]}, {"text": "tion. In PP-OCR, the dimension of the sequence features is", "confidence": 0.9907487630844116, "text_region": [[882.0, 823.0], [1555.0, 825.0], [1554.0, 857.0], [882.0, 855.0]]}, {"text": "set to 48 empirically.", "confidence": 0.9977990984916687, "text_region": [[885.0, 857.0], [1125.0, 857.0], [1125.0, 887.0], [885.0, 887.0]]}], "img_idx": 0, "score": 0.9834896326065063}
{"type": "text", "bbox": [887, 1191, 1555, 1346], "res": [{"text": "PACT Quantization We adopt the similar quantization", "confidence": 0.9995871782302856, "text_region": [[912.0, 1194.0], [1552.0, 1194.0], [1552.0, 1224.0], [912.0, 1224.0]]}, {"text": "scheme of the direction classification to reduce the model", "confidence": 0.9980757236480713, "text_region": [[889.0, 1228.0], [1552.0, 1228.0], [1552.0, 1251.0], [889.0, 1251.0]]}, {"text": "size of a text recognizer except for skipping the LSTM lay-", "confidence": 0.9771996736526489, "text_region": [[887.0, 1256.0], [1548.0, 1256.0], [1548.0, 1286.0], [887.0, 1286.0]]}, {"text": "ers. Those layers will not be quantified at present since the", "confidence": 0.983027994632721, "text_region": [[887.0, 1286.0], [1552.0, 1286.0], [1552.0, 1315.0], [887.0, 1315.0]]}, {"text": "complexity of LSTM quantization.", "confidence": 0.9813961982727051, "text_region": [[885.0, 1315.0], [1273.0, 1315.0], [1273.0, 1345.0], [885.0, 1345.0]]}], "img_idx": 0, "score": 0.9749465584754944}
{"type": "text", "bbox": [887, 488, 1555, 612], "res": [{"text": "ence on the accuracy.", "confidence": 0.9698877930641174, "text_region": [[887.0, 458.0], [1123.0, 461.0], [1122.0, 491.0], [887.0, 488.0]]}, {"text": "Learning Rate Warm-up Similar as the text detection,", "confidence": 0.9963741898536682, "text_region": [[912.0, 490.0], [1552.0, 490.0], [1552.0, 520.0], [912.0, 520.0]]}, {"text": "learning rate warm-up is also helping the text recognition.", "confidence": 0.9936280250549316, "text_region": [[885.0, 522.0], [1550.0, 522.0], [1550.0, 552.0], [885.0, 552.0]]}, {"text": "For text recognition, the experiments show that using this", "confidence": 0.9976486563682556, "text_region": [[887.0, 552.0], [1554.0, 552.0], [1554.0, 582.0], [887.0, 582.0]]}, {"text": "strategy is also effective.", "confidence": 0.9808313846588135, "text_region": [[885.0, 582.0], [1162.0, 582.0], [1162.0, 612.0], [885.0, 612.0]]}], "img_idx": 0, "score": 0.9666987657546997}
{"type": "text", "bbox": [149, 1306, 819, 1462], "res": [{"text": "BDA and TIA to the training images of the text recognition.", "confidence": 0.9972803592681885, "text_region": [[148.0, 1276.0], [813.0, 1276.0], [813.0, 1306.0], [148.0, 1306.0]]}, {"text": "Cosine Learning Rate Decay As mentioned in text de-", "confidence": 0.9992209672927856, "text_region": [[176.0, 1309.0], [808.0, 1309.0], [808.0, 1338.0], [176.0, 1338.0]]}, {"text": "tection, cosine learning rate decay has become the preferred", "confidence": 0.9903215765953064, "text_region": [[148.0, 1341.0], [815.0, 1341.0], [815.0, 1370.0], [148.0, 1370.0]]}, {"text": "learning rate reduction method. The experiments show that", "confidence": 0.9818042516708374, "text_region": [[146.0, 1368.0], [815.0, 1373.0], [815.0, 1403.0], [145.0, 1398.0]]}, {"text": "cosine learning rate decay strategy is also effective to en-", "confidence": 0.9923418164253235, "text_region": [[148.0, 1402.0], [813.0, 1402.0], [813.0, 1432.0], [148.0, 1432.0]]}, {"text": "hance the model ability for text recognition.", "confidence": 0.9988996982574463, "text_region": [[146.0, 1432.0], [638.0, 1432.0], [638.0, 1462.0], [146.0, 1462.0]]}], "img_idx": 0, "score": 0.9509152173995972}
{"type": "text", "bbox": [885, 1466, 1558, 1561], "res": [{"text": "DataSetsAs shown in Table1 in order to implement a", "confidence": 0.9911878108978271, "text_region": [[885.0, 1469.0], [1552.0, 1469.0], [1552.0, 1499.0], [885.0, 1499.0]]}, {"text": "practical OCR system, we construct a large-scale dataset for", "confidence": 0.9983043670654297, "text_region": [[882.0, 1501.0], [1552.0, 1499.0], [1552.0, 1529.0], [882.0, 1531.0]]}, {"text": "Chinese and English recognition as an example.", "confidence": 0.9955457448959351, "text_region": [[887.0, 1529.0], [1418.0, 1529.0], [1418.0, 1558.0], [887.0, 1558.0]]}, {"text": "For text detection, there are 97k training images and 500", "confidence": 0.9997591972351074, "text_region": [[912.0, 1561.0], [1552.0, 1561.0], [1552.0, 1590.0], [912.0, 1590.0]]}], "img_idx": 0, "score": 0.9427338242530823}
{"type": "text", "bbox": [150, 1051, 814, 1112], "res": [{"text": "Figure 12: Illustration of the modify of the feature map reso-", "confidence": 0.9778986573219299, "text_region": [[148.0, 1050.0], [809.0, 1054.0], [808.0, 1084.0], [148.0, 1079.0]]}, {"text": "lution. The table comes from the paper (Howard et al. 2019)", "confidence": 0.9891571402549744, "text_region": [[148.0, 1084.0], [808.0, 1084.0], [808.0, 1114.0], [148.0, 1114.0]]}], "img_idx": 0, "score": 0.9402990341186523}
{"type": "text", "bbox": [147, 1181, 817, 1306], "res": [{"text": "11] at first, a set of fiducial points are initialized on the im", "confidence": 0.9793637990951538, "text_region": [[148.0, 1185.0], [808.0, 1185.0], [808.0, 1215.0], [148.0, 1215.0]]}, {"text": "age. Then move the points randomly to generate a new im", "confidence": 0.9906008839607239, "text_region": [[146.0, 1215.0], [808.0, 1215.0], [808.0, 1244.0], [146.0, 1244.0]]}, {"text": "age with the geometric transformation. In PP-OCR, we add", "confidence": 0.9993410110473633, "text_region": [[145.0, 1244.0], [815.0, 1242.0], [815.0, 1274.0], [146.0, 1276.0]]}, {"text": "BDA and TIA to the training images of the text recognition.", "confidence": 0.9972803592681885, "text_region": [[148.0, 1276.0], [813.0, 1276.0], [813.0, 1306.0], [148.0, 1306.0]]}], "img_idx": 0, "score": 0.9354336261749268}
{"type": "text", "bbox": [150, 602, 816, 666], "res": [{"text": "Figure 11: Illustration of data augmentation, TIA. This fig-", "confidence": 0.9886839389801025, "text_region": [[146.0, 600.0], [808.0, 603.0], [808.0, 635.0], [145.0, 632.0]]}, {"text": "ure comes from the paper (Luo et al. 2020)", "confidence": 0.9931540489196777, "text_region": [[146.0, 635.0], [628.0, 635.0], [628.0, 667.0], [146.0, 667.0]]}], "img_idx": 0, "score": 0.9347825646400452}
{"type": "text", "bbox": [158, 1893, 781, 1957], "res": [{"text": "to achieve the better performance empirically.", "confidence": 0.9886837005615234, "text_region": [[146.0, 1863.0], [661.0, 1863.0], [661.0, 1893.0], [146.0, 1893.0]]}, {"text": "Regularization Parameters Overfitting is a common", "confidence": 0.9957962036132812, "text_region": [[176.0, 1898.0], [813.0, 1898.0], [813.0, 1927.0], [176.0, 1927.0]]}, {"text": "term in machine learning. A simple understanding is that", "confidence": 0.980601966381073, "text_region": [[148.0, 1932.0], [815.0, 1932.0], [815.0, 1955.0], [148.0, 1955.0]]}], "img_idx": 0, "score": 0.8384655714035034}
{"type": "title", "bbox": [887, 1423, 1212, 1456], "res": [{"text": "3.1Experimental Setup", "confidence": 0.9880654215812683, "text_region": [[882.0, 1423.0], [1217.0, 1425.0], [1217.0, 1458.0], [882.0, 1455.0]]}], "img_idx": 0, "score": 0.8864706754684448}
{"type": "title", "bbox": [1100, 1374, 1335, 1411], "res": [{"text": "3Experiments", "confidence": 0.9999179244041443, "text_region": [[1097.0, 1377.0], [1335.0, 1377.0], [1335.0, 1409.0], [1097.0, 1409.0]]}], "img_idx": 0, "score": 0.8785772323608398}
{"type": "figure", "bbox": [159, 157, 800, 568], "res": [{"text": "Input", "confidence": 0.9996944665908813, "text_region": [[298.0, 158.0], [358.0, 158.0], [358.0, 183.0], [298.0, 183.0]]}, {"text": "Augmented Image", "confidence": 0.9998722672462463, "text_region": [[598.0, 172.0], [785.0, 174.0], [785.0, 206.0], [598.0, 204.0]]}, {"text": "Safety", "confidence": 0.9991681575775146, "text_region": [[219.0, 185.0], [442.0, 193.0], [439.0, 257.0], [216.0, 249.0]]}, {"text": "Safety", "confidence": 0.9978135228157043, "text_region": [[598.0, 208.0], [791.0, 219.0], [788.0, 276.0], [595.0, 265.0]]}, {"text": "N=3", "confidence": 0.9921693205833435, "text_region": [[261.0, 264.0], [337.0, 264.0], [337.0, 289.0], [261.0, 289.0]]}, {"text": "points", "confidence": 0.9978935718536377, "text_region": [[501.0, 259.0], [552.0, 259.0], [552.0, 286.0], [501.0, 286.0]]}, {"text": "following", "confidence": 0.9985575079917908, "text_region": [[494.0, 274.0], [560.0, 283.0], [557.0, 308.0], [491.0, 299.0]]}, {"text": "Safety", "confidence": 0.996790885925293, "text_region": [[593.0, 290.0], [781.0, 281.0], [784.0, 345.0], [596.0, 354.0]]}, {"text": "Safety", "confidence": 0.9543476104736328, "text_region": [[215.0, 311.0], [451.0, 322.0], [448.0, 395.0], [211.0, 384.0]]}, {"text": "distribution.", "confidence": 0.996238112449646, "text_region": [[483.0, 328.0], [568.0, 328.0], [568.0, 351.0], [483.0, 351.0]]}, {"text": "safety", "confidence": 0.9926063418388367, "text_region": [[598.0, 358.0], [792.0, 380.0], [785.0, 445.0], [590.0, 422.0]]}, {"text": "Safety", "confidence": 0.998746931552887, "text_region": [[591.0, 431.0], [794.0, 456.0], [787.0, 515.0], [584.0, 491.0]]}, {"text": "Safety", "confidence": 0.9939134120941162, "text_region": [[211.0, 455.0], [445.0, 466.0], [441.0, 547.0], [207.0, 535.0]]}, {"text": ":", "confidence": 0.9996010661125183, "text_region": [[672.0, 527.0], [691.0, 527.0], [691.0, 555.0], [672.0, 555.0]]}], "img_idx": 0, "score": 0.9709455370903015}
{"type": "figure", "bbox": [196, 706, 786, 1021], "res": [{"text": "Input", "confidence": 0.9981124997138977, "text_region": [[217.0, 745.0], [243.0, 745.0], [243.0, 761.0], [217.0, 761.0]]}, {"text": "6*160", "confidence": 0.9166690111160278, "text_region": [[725.0, 777.0], [760.0, 777.0], [760.0, 793.0], [725.0, 793.0]]}, {"text": "16", "confidence": 0.9952212572097778, "text_region": [[236.0, 788.0], [254.0, 788.0], [254.0, 809.0], [236.0, 809.0]]}, {"text": " 24", "confidence": 0.9569501876831055, "text_region": [[226.0, 811.0], [254.0, 811.0], [254.0, 827.0], [226.0, 827.0]]}, {"text": "2.14*160", "confidence": 0.9082335233688354, "text_region": [[688.0, 823.0], [753.0, 823.0], [753.0, 839.0], [688.0, 839.0]]}, {"text": "42", "confidence": 0.9889794588088989, "text_region": [[210.0, 871.0], [224.0, 871.0], [224.0, 891.0], [210.0, 891.0]]}, {"text": "40", "confidence": 0.9982301592826843, "text_region": [[236.0, 869.0], [254.0, 869.0], [254.0, 891.0], [236.0, 891.0]]}, {"text": "21*80", "confidence": 0.9516789317131042, "text_region": [[698.0, 965.0], [744.0, 965.0], [744.0, 981.0], [698.0, 981.0]]}, {"text": "bandoned", "confidence": 0.9974358081817627, "text_region": [[612.0, 990.0], [679.0, 990.0], [679.0, 1013.0], [612.0, 1013.0]]}], "img_idx": 0, "score": 0.9021558165550232}
