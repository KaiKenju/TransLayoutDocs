{"type": "text", "bbox": [150, 900, 816, 1267], "res": [{"text": "bols recognition.", "confidence": 0.9998981356620789, "text_region": [[148.0, 873.0], [340.0, 873.0], [340.0, 903.0], [148.0, 903.0]]}, {"text": "In order to implement a practical OCR system, we con-", "confidence": 0.9919371604919434, "text_region": [[173.0, 903.0], [813.0, 903.0], [813.0, 933.0], [173.0, 933.0]]}, {"text": "struct a large-scale dataset for Chinese and English recog.", "confidence": 0.9777150750160217, "text_region": [[148.0, 935.0], [808.0, 935.0], [808.0, 965.0], [148.0, 965.0]]}, {"text": "nition as an example. Specifically, text detection dataset has", "confidence": 0.9933449029922485, "text_region": [[148.0, 965.0], [815.0, 965.0], [815.0, 995.0], [148.0, 995.0]]}, {"text": "97K images. Direction classification dataset has 600k im", "confidence": 0.9902786612510681, "text_region": [[148.0, 995.0], [806.0, 995.0], [806.0, 1024.0], [148.0, 1024.0]]}, {"text": "ages. Text recognition dataset has 17.9M images. A small", "confidence": 0.9938269853591919, "text_region": [[145.0, 1024.0], [815.0, 1022.0], [815.0, 1054.0], [146.0, 1056.0]]}, {"text": "amount of the data are selected to conduct ablation exper-", "confidence": 0.9972910284996033, "text_region": [[150.0, 1059.0], [811.0, 1059.0], [811.0, 1082.0], [150.0, 1082.0]]}, {"text": "iments quickly and choose the appropriate strategies. We", "confidence": 0.9922000169754028, "text_region": [[148.0, 1086.0], [815.0, 1086.0], [815.0, 1116.0], [148.0, 1116.0]]}, {"text": "make a lot of ablation experiments to show the effects of", "confidence": 0.9964534640312195, "text_region": [[148.0, 1116.0], [815.0, 1116.0], [815.0, 1146.0], [148.0, 1146.0]]}, {"text": "different strategies in Figure2 Besides, we also verify the", "confidence": 0.9906630516052246, "text_region": [[148.0, 1148.0], [813.0, 1148.0], [813.0, 1178.0], [148.0, 1178.0]]}, {"text": "proposed PP-OCR system for other languages recognition", "confidence": 0.9861560463905334, "text_region": [[148.0, 1178.0], [813.0, 1178.0], [813.0, 1208.0], [148.0, 1208.0]]}, {"text": "which including alphanumeric symbols, French, Korean,", "confidence": 0.9892807602882385, "text_region": [[148.0, 1208.0], [813.0, 1208.0], [813.0, 1238.0], [148.0, 1238.0]]}, {"text": "Japanese and German.", "confidence": 0.9832679033279419, "text_region": [[148.0, 1238.0], [402.0, 1238.0], [402.0, 1267.0], [148.0, 1267.0]]}], "img_idx": 0, "score": 0.9824100732803345}
{"type": "text", "bbox": [887, 1464, 1554, 1740], "res": [{"text": "MobileNetV3 can achieve higher accuracy when the pre", "confidence": 0.989064633846283, "text_region": [[882.0, 1464.0], [1545.0, 1467.0], [1545.0, 1496.0], [882.0, 1494.0]]}, {"text": "dict time are same. As for the choice of scale, we adopt", "confidence": 0.9980286359786987, "text_region": [[885.0, 1496.0], [1552.0, 1496.0], [1552.0, 1526.0], [885.0, 1526.0]]}, {"text": "MobileNetV3_large_x0.5 to balance accuracy and efficiency", "confidence": 0.9727887511253357, "text_region": [[885.0, 1526.0], [1548.0, 1526.0], [1548.0, 1556.0], [885.0, 1556.0]]}, {"text": "empirically. Incidentally, PaddleClas provides a total of up", "confidence": 0.9875838160514832, "text_region": [[887.0, 1558.0], [1552.0, 1558.0], [1552.0, 1588.0], [887.0, 1588.0]]}, {"text": "to 24 series of image classification network structures and", "confidence": 0.9853535890579224, "text_region": [[885.0, 1588.0], [1552.0, 1588.0], [1552.0, 1618.0], [885.0, 1618.0]]}, {"text": "training configurations, 122 models' pretrained weights and", "confidence": 0.9951123595237732, "text_region": [[885.0, 1618.0], [1552.0, 1618.0], [1552.0, 1648.0], [885.0, 1648.0]]}, {"text": "their evaluation metrics, such as ResNet, ResNet_vd, SERes-", "confidence": 0.9988927841186523, "text_region": [[889.0, 1652.0], [1548.0, 1652.0], [1548.0, 1675.0], [889.0, 1675.0]]}, {"text": "NeXt, Res2Net, Res2Net_vd, DPN, DenseNet, EfficientNet,", "confidence": 0.9904300570487976, "text_region": [[889.0, 1682.0], [1550.0, 1682.0], [1550.0, 1705.0], [889.0, 1705.0]]}, {"text": "Xception, HRNet, etc.", "confidence": 0.9658998847007751, "text_region": [[883.0, 1707.0], [1134.0, 1710.0], [1134.0, 1742.0], [882.0, 1739.0]]}], "img_idx": 0, "score": 0.976142942905426}
{"type": "text", "bbox": [887, 1741, 1554, 1956], "res": [{"text": "Xception, HRNet, etc.", "confidence": 0.9658998847007751, "text_region": [[883.0, 1707.0], [1134.0, 1710.0], [1134.0, 1742.0], [882.0, 1739.0]]}, {"text": "Light Head The head of the text detector is similar as", "confidence": 0.9988711476325989, "text_region": [[910.0, 1742.0], [1552.0, 1744.0], [1552.0, 1774.0], [910.0, 1771.0]]}, {"text": "the FPN (Lin et al.2017) architecture in object detection", "confidence": 0.9964050650596619, "text_region": [[885.0, 1774.0], [1552.0, 1774.0], [1552.0, 1804.0], [885.0, 1804.0]]}, {"text": "and fuse the feature maps of the different scales to im-", "confidence": 0.9877491593360901, "text_region": [[885.0, 1806.0], [1550.0, 1806.0], [1550.0, 1836.0], [885.0, 1836.0]]}, {"text": "prove the effect for the small text regions detection. For con-", "confidence": 0.9983318448066711, "text_region": [[887.0, 1836.0], [1548.0, 1836.0], [1548.0, 1865.0], [887.0, 1865.0]]}, {"text": "venience of merging the different resolution feature maps,", "confidence": 0.9933119416236877, "text_region": [[885.0, 1868.0], [1550.0, 1868.0], [1550.0, 1898.0], [885.0, 1898.0]]}, {"text": "1  1 convolution is often used to reduce the feature maps", "confidence": 0.9802920818328857, "text_region": [[882.0, 1895.0], [1552.0, 1898.0], [1552.0, 1927.0], [882.0, 1925.0]]}, {"text": "to the same number of channel (we use inner channels for", "confidence": 0.9765847325325012, "text_region": [[882.0, 1925.0], [1552.0, 1927.0], [1552.0, 1957.0], [882.0, 1955.0]]}], "img_idx": 0, "score": 0.9717795848846436}
{"type": "text", "bbox": [150, 1627, 818, 1902], "res": [{"text": "text detector DB.", "confidence": 0.9998658299446106, "text_region": [[148.0, 1597.0], [340.0, 1597.0], [340.0, 1627.0], [148.0, 1627.0]]}, {"text": "Light Backbone The size of backbone has dominant", "confidence": 0.9952408671379089, "text_region": [[176.0, 1629.0], [815.0, 1629.0], [815.0, 1659.0], [176.0, 1659.0]]}, {"text": "effect on the model size of a text detector. Therefore,", "confidence": 0.9872543215751648, "text_region": [[148.0, 1659.0], [813.0, 1659.0], [813.0, 1689.0], [148.0, 1689.0]]}, {"text": "light backbones should be selected for building the ultra", "confidence": 0.9942042827606201, "text_region": [[150.0, 1694.0], [813.0, 1694.0], [813.0, 1716.0], [150.0, 1716.0]]}, {"text": "lightweight models. With the development of image clas.", "confidence": 0.9963276982307434, "text_region": [[148.0, 1719.0], [808.0, 1719.0], [808.0, 1749.0], [148.0, 1749.0]]}, {"text": "sification, MobileNetV1, MobileNetV2, MobileNetV3 and", "confidence": 0.9910639524459839, "text_region": [[148.0, 1751.0], [813.0, 1751.0], [813.0, 1781.0], [148.0, 1781.0]]}, {"text": "ShuffleNetV2 series are often used as the light backbones.", "confidence": 0.9897112250328064, "text_region": [[148.0, 1781.0], [813.0, 1781.0], [813.0, 1810.0], [148.0, 1810.0]]}, {"text": "Each series has different scale. Thanks to the inference", "confidence": 0.9924203157424927, "text_region": [[150.0, 1813.0], [815.0, 1813.0], [815.0, 1842.0], [150.0, 1842.0]]}, {"text": "time on CPU and accuracy of more than 20 kinds of back", "confidence": 0.9986315369606018, "text_region": [[150.0, 1842.0], [806.0, 1842.0], [806.0, 1872.0], [150.0, 1872.0]]}, {"text": "bones are provided by PaddleClas as shown in Figure6", "confidence": 0.9947137236595154, "text_region": [[148.0, 1872.0], [808.0, 1872.0], [808.0, 1904.0], [148.0, 1904.0]]}], "img_idx": 0, "score": 0.9699578285217285}
{"type": "text", "bbox": [150, 748, 816, 902], "res": [{"text": "tion, regularization parameters, learning rate warm-up, light", "confidence": 0.9851953983306885, "text_region": [[150.0, 749.0], [811.0, 749.0], [811.0, 779.0], [150.0, 779.0]]}, {"text": "head, pre-trained model and PACT quantization. Finally, the", "confidence": 0.9991354942321777, "text_region": [[148.0, 779.0], [811.0, 779.0], [811.0, 809.0], [148.0, 809.0]]}, {"text": "model size of the text recognizer is only 1.6M for Chinese", "confidence": 0.9993225336074829, "text_region": [[148.0, 811.0], [813.0, 811.0], [813.0, 841.0], [148.0, 841.0]]}, {"text": "and English recognition and 900KB for alphanumeric sym-", "confidence": 0.9923476576805115, "text_region": [[148.0, 843.0], [811.0, 843.0], [811.0, 873.0], [148.0, 873.0]]}, {"text": "bols recognition.", "confidence": 0.9998981356620789, "text_region": [[148.0, 873.0], [340.0, 873.0], [340.0, 903.0], [148.0, 903.0]]}], "img_idx": 0, "score": 0.967653214931488}
{"type": "text", "bbox": [887, 1242, 1554, 1395], "res": [{"text": "Figure 6: The performance of some light backbones on the", "confidence": 0.9997652173042297, "text_region": [[887.0, 1244.0], [1552.0, 1244.0], [1552.0, 1274.0], [887.0, 1274.0]]}, {"text": "ImageNet 1000 classification, including MobileNetV1, Mo-", "confidence": 0.9768468141555786, "text_region": [[887.0, 1279.0], [1548.0, 1279.0], [1548.0, 1302.0], [887.0, 1302.0]]}, {"text": "bileNetV2, MobileNetV3 and ShuffleNetV2 series. The in-", "confidence": 0.9975768327713013, "text_region": [[885.0, 1306.0], [1550.0, 1306.0], [1550.0, 1336.0], [885.0, 1336.0]]}, {"text": "ference time is tested on Snapdragon 855 (SD855) with the", "confidence": 0.9935444593429565, "text_region": [[885.0, 1336.0], [1552.0, 1336.0], [1552.0, 1366.0], [885.0, 1366.0]]}, {"text": "batch size set as 1.", "confidence": 0.9998345375061035, "text_region": [[885.0, 1368.0], [1095.0, 1368.0], [1095.0, 1398.0], [885.0, 1398.0]]}], "img_idx": 0, "score": 0.9660409688949585}
{"type": "text", "bbox": [150, 1504, 818, 1627], "res": [{"text": "In this section, the details of six strategies for enhancing the", "confidence": 0.9988975524902344, "text_region": [[148.0, 1508.0], [813.0, 1508.0], [813.0, 1538.0], [148.0, 1538.0]]}, {"text": "model ability or reducing the model size of a text detector", "confidence": 0.9995472431182861, "text_region": [[148.0, 1538.0], [815.0, 1538.0], [815.0, 1568.0], [148.0, 1568.0]]}, {"text": "will be introduced. Figure 5 shows the architecture of the", "confidence": 0.9823813438415527, "text_region": [[148.0, 1570.0], [815.0, 1570.0], [815.0, 1600.0], [148.0, 1600.0]]}, {"text": "text detector DB.", "confidence": 0.9998658299446106, "text_region": [[148.0, 1597.0], [340.0, 1597.0], [340.0, 1627.0], [148.0, 1627.0]]}], "img_idx": 0, "score": 0.953559160232544}
{"type": "text", "bbox": [150, 1266, 817, 1390], "res": [{"text": "Japanese and German.", "confidence": 0.9832679033279419, "text_region": [[148.0, 1238.0], [402.0, 1238.0], [402.0, 1267.0], [148.0, 1267.0]]}, {"text": "The rest of the paper is organized as follows. In section", "confidence": 0.9779001474380493, "text_region": [[173.0, 1270.0], [815.0, 1270.0], [815.0, 1299.0], [173.0, 1299.0]]}, {"text": "2, we present the bag of model enhancement or slimming", "confidence": 0.9930790066719055, "text_region": [[146.0, 1299.0], [815.0, 1299.0], [815.0, 1329.0], [146.0, 1329.0]]}, {"text": "strategies. Experimental results are discussed in section 3", "confidence": 0.9838521480560303, "text_region": [[150.0, 1334.0], [813.0, 1334.0], [813.0, 1357.0], [150.0, 1357.0]]}, {"text": "and conclusion is conducted in section 4.", "confidence": 0.9999314546585083, "text_region": [[146.0, 1361.0], [603.0, 1361.0], [603.0, 1384.0], [146.0, 1384.0]]}], "img_idx": 0, "score": 0.9404134154319763}
{"type": "text", "bbox": [226, 624, 1472, 690], "res": [{"text": "Figure 5: Architecture of the text detector DB. This figure comes from the paper of DB (Liao et al. 2020). The red and gray", "confidence": 0.9879029989242554, "text_region": [[146.0, 623.0], [1550.0, 628.0], [1550.0, 660.0], [145.0, 655.0]]}, {"text": "rectangles show the backbone and head of the text detector separately.", "confidence": 0.9975959658622742, "text_region": [[148.0, 662.0], [924.0, 662.0], [924.0, 685.0], [148.0, 685.0]]}], "img_idx": 0, "score": 0.8307771682739258}
{"type": "title", "bbox": [186, 1416, 774, 1456], "res": [{"text": "2Enhancement or Slimming Strategies", "confidence": 0.9909299612045288, "text_region": [[185.0, 1418.0], [772.0, 1423.0], [771.0, 1455.0], [185.0, 1451.0]]}], "img_idx": 0, "score": 0.6718987226486206}
{"type": "figure", "bbox": [290, 160, 1410, 597], "res": [{"text": "Head", "confidence": 0.9998404383659363, "text_region": [[739.0, 158.0], [822.0, 158.0], [822.0, 192.0], [739.0, 192.0]]}, {"text": "conv", "confidence": 0.9872115850448608, "text_region": [[654.0, 218.0], [686.0, 218.0], [686.0, 234.0], [654.0, 234.0]]}, {"text": "conv, up4", "confidence": 0.9548278450965881, "text_region": [[695.0, 275.0], [771.0, 275.0], [771.0, 298.0], [695.0, 298.0]]}, {"text": "pN Up-sample with ratio N", "confidence": 0.9535754323005676, "text_region": [[303.0, 286.0], [504.0, 286.0], [504.0, 302.0], [303.0, 302.0]]}, {"text": "conv,up8", "confidence": 0.9930121898651123, "text_region": [[686.0, 309.0], [771.0, 309.0], [771.0, 332.0], [686.0, 332.0]]}, {"text": "approximate", "confidence": 0.998657763004303, "text_region": [[1134.0, 401.0], [1217.0, 401.0], [1217.0, 424.0], [1134.0, 424.0]]}, {"text": "binary map", "confidence": 0.9611495137214661, "text_region": [[1136.0, 419.0], [1215.0, 419.0], [1215.0, 442.0], [1136.0, 442.0]]}, {"text": "1/16", "confidence": 0.9953314661979675, "text_region": [[642.0, 520.0], [681.0, 520.0], [681.0, 536.0], [642.0, 536.0]]}, {"text": "Backbone", "confidence": 0.9996916651725769, "text_region": [[513.0, 564.0], [656.0, 564.0], [656.0, 596.0], [513.0, 596.0]]}], "img_idx": 0, "score": 0.9691417217254639}
{"type": "figure", "bbox": [909, 777, 1542, 1203], "res": [{"text": " Performance of the mobile models", "confidence": 0.9866929650306702, "text_region": [[1021.0, 775.0], [1298.0, 775.0], [1298.0, 798.0], [1021.0, 798.0]]}, {"text": "0.80", "confidence": 0.9995802640914917, "text_region": [[931.0, 800.0], [965.0, 800.0], [965.0, 823.0], [931.0, 823.0]]}, {"text": "*", "confidence": 0.9701789617538452, "text_region": [[1118.0, 814.0], [1129.0, 814.0], [1129.0, 830.0], [1118.0, 830.0]]}, {"text": "x", "confidence": 0.7303844690322876, "text_region": [[1217.0, 825.0], [1236.0, 825.0], [1236.0, 846.0], [1217.0, 846.0]]}, {"text": "0.75", "confidence": 0.9781584143638611, "text_region": [[929.0, 852.0], [965.0, 852.0], [965.0, 880.0], [929.0, 880.0]]}, {"text": "0.70", "confidence": 0.9996857047080994, "text_region": [[929.0, 910.0], [965.0, 910.0], [965.0, 937.0], [929.0, 937.0]]}, {"text": "0.60", "confidence": 0.909563422203064, "text_region": [[929.0, 1027.0], [965.0, 1027.0], [965.0, 1054.0], [929.0, 1054.0]]}, {"text": "0.55", "confidence": 0.879755973815918, "text_region": [[931.0, 1084.0], [965.0, 1084.0], [965.0, 1111.0], [931.0, 1111.0]]}, {"text": "*!", "confidence": 0.828635573387146, "text_region": [[984.0, 1107.0], [1005.0, 1107.0], [1005.0, 1128.0], [984.0, 1128.0]]}, {"text": "0.50", "confidence": 0.9997045993804932, "text_region": [[933.0, 1146.0], [963.0, 1146.0], [963.0, 1164.0], [933.0, 1164.0]]}, {"text": "Inference time of batchsize=1(ms, SD855)", "confidence": 0.969535231590271, "text_region": [[1016.0, 1176.0], [1303.0, 1178.0], [1303.0, 1208.0], [1016.0, 1205.0]]}], "img_idx": 0, "score": 0.9455599784851074}
