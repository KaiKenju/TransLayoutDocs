{"type": "text", "bbox": [887, 1367, 1555, 1673], "res": [{"text": "easy to be achieved by geometric transformation as the de-", "confidence": 0.9967383146286011, "text_region": [[885.0, 1368.0], [1552.0, 1368.0], [1552.0, 1398.0], [885.0, 1398.0]]}, {"text": "tection frame is composed of four points. However, the rec-", "confidence": 0.9991198778152466, "text_region": [[885.0, 1400.0], [1548.0, 1400.0], [1548.0, 1430.0], [885.0, 1430.0]]}, {"text": "tified boxes may be reversed. Thus, a classifier is needed", "confidence": 0.9850175976753235, "text_region": [[885.0, 1430.0], [1552.0, 1430.0], [1552.0, 1460.0], [885.0, 1460.0]]}, {"text": "to determine the text direction. If a box is determined re-", "confidence": 0.9923799633979797, "text_region": [[885.0, 1460.0], [1552.0, 1460.0], [1552.0, 1490.0], [885.0, 1490.0]]}, {"text": "versed, further flipping is required. Training a text direction", "confidence": 0.9967222809791565, "text_region": [[885.0, 1492.0], [1552.0, 1492.0], [1552.0, 1522.0], [885.0, 1522.0]]}, {"text": "classifier is a simple image classification task. We adopt the", "confidence": 0.9985648989677429, "text_region": [[887.0, 1522.0], [1552.0, 1522.0], [1552.0, 1551.0], [887.0, 1551.0]]}, {"text": "following four strategies to enhance the model ability and", "confidence": 0.9996406435966492, "text_region": [[887.0, 1551.0], [1554.0, 1551.0], [1554.0, 1581.0], [887.0, 1581.0]]}, {"text": "reduce the model size: light backbone, data augmentation,", "confidence": 0.9906911253929138, "text_region": [[882.0, 1581.0], [1548.0, 1584.0], [1547.0, 1613.0], [882.0, 1611.0]]}, {"text": "input resolution and PACT quantization. Finally, the model.", "confidence": 0.9903615117073059, "text_region": [[887.0, 1616.0], [1552.0, 1616.0], [1552.0, 1639.0], [887.0, 1639.0]]}, {"text": "size of the text direction classifier is 50oKB.", "confidence": 0.9826754331588745, "text_region": [[887.0, 1645.0], [1377.0, 1645.0], [1377.0, 1668.0], [887.0, 1668.0]]}], "img_idx": 0, "score": 0.9829190373420715}
{"type": "text", "bbox": [887, 1680, 1556, 1957], "res": [{"text": "Text Recognition In PP-OCR, we use CRNN (Shi, Bai,", "confidence": 0.9955587983131409, "text_region": [[915.0, 1682.0], [1552.0, 1682.0], [1552.0, 1712.0], [915.0, 1712.0]]}, {"text": "and Yao 2016) as text recognizer, which is widely used and", "confidence": 0.9898862242698669, "text_region": [[882.0, 1714.0], [1552.0, 1714.0], [1552.0, 1744.0], [882.0, 1744.0]]}, {"text": "practical for text recognition. CRNN integrates feature ex-", "confidence": 0.9996980428695679, "text_region": [[885.0, 1744.0], [1547.0, 1742.0], [1548.0, 1774.0], [885.0, 1776.0]]}, {"text": "traction and sequence modeling. It adopts the Connection-.", "confidence": 0.9854505062103271, "text_region": [[887.0, 1778.0], [1550.0, 1778.0], [1550.0, 1801.0], [887.0, 1801.0]]}, {"text": "ist Temporal Classification(CTC) loss to avoid the inconsis.", "confidence": 0.9941261410713196, "text_region": [[885.0, 1806.0], [1545.0, 1806.0], [1545.0, 1836.0], [885.0, 1836.0]]}, {"text": "tency between prediction and label. To enhance the model", "confidence": 0.9984685778617859, "text_region": [[887.0, 1836.0], [1552.0, 1836.0], [1552.0, 1865.0], [887.0, 1865.0]]}, {"text": "ability and reduce the model size of a text recognizer, the.", "confidence": 0.9899963736534119, "text_region": [[889.0, 1870.0], [1550.0, 1870.0], [1550.0, 1893.0], [889.0, 1893.0]]}, {"text": "following nine strategies are used: light backbone, data aug-", "confidence": 0.9916780591011047, "text_region": [[887.0, 1898.0], [1548.0, 1898.0], [1548.0, 1927.0], [887.0, 1927.0]]}, {"text": "mentation, cosine learning rate decay, feature map resolu-.", "confidence": 0.9839876890182495, "text_region": [[887.0, 1932.0], [1545.0, 1932.0], [1545.0, 1955.0], [887.0, 1955.0]]}], "img_idx": 0, "score": 0.9807730317115784}
{"type": "text", "bbox": [150, 1311, 818, 1556], "res": [{"text": "considering the cost. In particular, the OCR system need to", "confidence": 0.9962257146835327, "text_region": [[148.0, 1313.0], [815.0, 1313.0], [815.0, 1343.0], [148.0, 1343.0]]}, {"text": "be run on embedded devices in many scenarios, such as cell", "confidence": 0.9997403621673584, "text_region": [[146.0, 1343.0], [815.0, 1343.0], [815.0, 1373.0], [146.0, 1373.0]]}, {"text": "phones, which makes it necessary to consider the model size.", "confidence": 0.9913354516029358, "text_region": [[148.0, 1373.0], [813.0, 1373.0], [813.0, 1402.0], [148.0, 1402.0]]}, {"text": "Trade off model size and performance is difficult but of great", "confidence": 0.9889301061630249, "text_region": [[148.0, 1405.0], [815.0, 1405.0], [815.0, 1435.0], [148.0, 1435.0]]}, {"text": "value. In this paper, we propose a practical ultra lightweight", "confidence": 0.9950801134109497, "text_region": [[148.0, 1435.0], [815.0, 1435.0], [815.0, 1464.0], [148.0, 1464.0]]}, {"text": "OCR system, named as PP-OCR, which consists of three", "confidence": 0.9864562749862671, "text_region": [[148.0, 1464.0], [815.0, 1464.0], [815.0, 1494.0], [148.0, 1494.0]]}, {"text": "parts, text detection, detected boxes rectification and text.", "confidence": 0.9914132952690125, "text_region": [[150.0, 1499.0], [813.0, 1499.0], [813.0, 1522.0], [150.0, 1522.0]]}, {"text": "recognition as shown in Figure2", "confidence": 0.9840618371963501, "text_region": [[146.0, 1526.0], [520.0, 1526.0], [520.0, 1556.0], [146.0, 1556.0]]}], "img_idx": 0, "score": 0.9783332943916321}
{"type": "text", "bbox": [150, 1557, 817, 1864], "res": [{"text": "Text Detection The purpose of text detection is to locate", "confidence": 0.9999404549598694, "text_region": [[176.0, 1558.0], [815.0, 1558.0], [815.0, 1588.0], [176.0, 1588.0]]}, {"text": "the text area in the image. In PP-OCR, we use Differentiable", "confidence": 0.9981587529182434, "text_region": [[148.0, 1588.0], [815.0, 1588.0], [815.0, 1618.0], [148.0, 1618.0]]}, {"text": "Binarization (DB) (Liao et al. 2020) as text detector which is", "confidence": 0.9771919250488281, "text_region": [[148.0, 1620.0], [815.0, 1620.0], [815.0, 1650.0], [148.0, 1650.0]]}, {"text": "based on a simple segmentation network. The simple post", "confidence": 0.9995790123939514, "text_region": [[146.0, 1648.0], [806.0, 1648.0], [806.0, 1678.0], [146.0, 1678.0]]}, {"text": "processing of DB makes it very efficient. In order to further", "confidence": 0.9862974286079407, "text_region": [[148.0, 1680.0], [815.0, 1680.0], [815.0, 1710.0], [148.0, 1710.0]]}, {"text": "improve its effectiveness and efficiency, the following six", "confidence": 0.9925428628921509, "text_region": [[146.0, 1712.0], [815.0, 1712.0], [815.0, 1742.0], [146.0, 1742.0]]}, {"text": "strategies are used: light backbone, light head, remove SE", "confidence": 0.993306577205658, "text_region": [[148.0, 1742.0], [813.0, 1742.0], [813.0, 1771.0], [148.0, 1771.0]]}, {"text": "module, cosine learning rate decay, learning rate warm-up,", "confidence": 0.9910310506820679, "text_region": [[146.0, 1769.0], [813.0, 1771.0], [813.0, 1804.0], [145.0, 1801.0]]}, {"text": "and FPGM pruner. Finally, the model size of the text detector", "confidence": 0.9957398176193237, "text_region": [[146.0, 1799.0], [815.0, 1801.0], [815.0, 1833.0], [145.0, 1831.0]]}, {"text": "is reduced to 1.4M.", "confidence": 0.9981017708778381, "text_region": [[146.0, 1833.0], [365.0, 1833.0], [365.0, 1863.0], [146.0, 1863.0]]}], "img_idx": 0, "score": 0.977486789226532}
{"type": "text", "bbox": [149, 737, 1557, 830], "res": [{"text": "Figure 2: The framework of the proposed PP-OCR. The model size in the figure is about Chinese and English characters", "confidence": 0.9923454523086548, "text_region": [[146.0, 736.0], [1552.0, 738.0], [1552.0, 770.0], [145.0, 768.0]]}, {"text": "recognition. For alphanumeric symbols recognition, the model size of text recognition is from 1.6M to 0.9M. The rest of the", "confidence": 0.9957625865936279, "text_region": [[148.0, 770.0], [1552.0, 770.0], [1552.0, 800.0], [148.0, 800.0]]}, {"text": "models are the same size.", "confidence": 0.979947030544281, "text_region": [[148.0, 800.0], [432.0, 800.0], [432.0, 830.0], [148.0, 830.0]]}], "img_idx": 0, "score": 0.9450289011001587}
{"type": "text", "bbox": [945, 1256, 1491, 1288], "res": [{"text": "Figure 4: Some images contained document text..", "confidence": 0.9682695865631104, "text_region": [[947.0, 1258.0], [1490.0, 1258.0], [1490.0, 1288.0], [947.0, 1288.0]]}], "img_idx": 0, "score": 0.9259317517280579}
{"type": "text", "bbox": [150, 1863, 818, 1957], "res": [{"text": "is reduced to 1.4M.", "confidence": 0.9981017708778381, "text_region": [[146.0, 1833.0], [365.0, 1833.0], [365.0, 1863.0], [146.0, 1863.0]]}, {"text": "Detection Boxes Rectify Before recognizing the detected", "confidence": 0.9979832768440247, "text_region": [[176.0, 1865.0], [815.0, 1865.0], [815.0, 1895.0], [176.0, 1895.0]]}, {"text": "text, the text box needs to be transformed into a horizon-", "confidence": 0.997380793094635, "text_region": [[148.0, 1898.0], [813.0, 1898.0], [813.0, 1927.0], [148.0, 1927.0]]}, {"text": "tal rectangle box for subsequent text recognition, which is", "confidence": 0.989660918712616, "text_region": [[143.0, 1925.0], [815.0, 1927.0], [815.0, 1957.0], [143.0, 1955.0]]}], "img_idx": 0, "score": 0.9199883341789246}
{"type": "figure", "bbox": [265, 155, 1435, 710], "res": [{"text": "ODMDEM", "confidence": 0.6186266541481018, "text_region": [[735.0, 183.0], [767.0, 183.0], [767.0, 309.0], [735.0, 309.0]]}, {"text": "ODMDEM", "confidence": 0.5971829295158386, "text_region": [[827.0, 222.0], [961.0, 222.0], [961.0, 254.0], [827.0, 254.0]]}, {"text": "ODM OEM", "confidence": 0.9133197069168091, "text_region": [[1093.0, 225.0], [1222.0, 225.0], [1222.0, 264.0], [1093.0, 264.0]]}, {"text": "ODM OEM", "confidence": 0.942230761051178, "text_region": [[834.0, 319.0], [954.0, 319.0], [954.0, 344.0], [834.0, 344.0]]}, {"text": "ODM OEM", "confidence": 0.9709687829017639, "text_region": [[1102.0, 321.0], [1213.0, 321.0], [1213.0, 346.0], [1102.0, 346.0]]}, {"text": " Image", "confidence": 0.9497879147529602, "text_region": [[280.0, 384.0], [352.0, 393.0], [348.0, 427.0], [276.0, 418.0]]}, {"text": "Text Detection", "confidence": 0.9994257092475891, "text_region": [[464.0, 380.0], [614.0, 380.0], [614.0, 406.0], [464.0, 406.0]]}, {"text": "Detection Boxes Rectify", "confidence": 0.9741412997245789, "text_region": [[730.0, 378.0], [965.0, 378.0], [965.0, 408.0], [730.0, 408.0]]}, {"text": "Text Recognition", "confidence": 0.9998854994773865, "text_region": [[1074.0, 380.0], [1243.0, 380.0], [1243.0, 406.0], [1074.0, 406.0]]}, {"text": "Output", "confidence": 0.999603807926178, "text_region": [[1342.0, 387.0], [1421.0, 387.0], [1421.0, 419.0], [1342.0, 419.0]]}, {"text": "( db_mv3_slim, 1.4M )", "confidence": 0.955433189868927, "text_region": [[439.0, 406.0], [649.0, 406.0], [649.0, 429.0], [439.0, 429.0]]}, {"text": "(dir_cls_mv3_slim, 0.5M)", "confidence": 0.964713990688324, "text_region": [[730.0, 408.0], [963.0, 408.0], [963.0, 431.0], [730.0, 431.0]]}, {"text": "(crnn_mv3_slim, 1.6M)", "confidence": 0.9988844990730286, "text_region": [[1046.0, 403.0], [1266.0, 403.0], [1266.0, 435.0], [1046.0, 435.0]]}, {"text": "1. Light Backbone", "confidence": 0.9998587369918823, "text_region": [[418.0, 454.0], [580.0, 454.0], [580.0, 479.0], [418.0, 479.0]]}, {"text": "1. Light Backbone", "confidence": 0.9997239708900452, "text_region": [[716.0, 458.0], [882.0, 461.0], [882.0, 486.0], [716.0, 483.0]]}, {"text": "1. Light Backbone", "confidence": 0.9998651146888733, "text_region": [[1042.0, 456.0], [1201.0, 456.0], [1201.0, 479.0], [1042.0, 479.0]]}, {"text": "2. Light Head", "confidence": 0.9675201773643494, "text_region": [[414.0, 476.0], [543.0, 482.0], [542.0, 514.0], [413.0, 508.0]]}, {"text": "2. Data Augmentation", "confidence": 0.999673068523407, "text_region": [[716.0, 486.0], [917.0, 491.0], [917.0, 516.0], [716.0, 511.0]]}, {"text": "2. Data Augmentation", "confidence": 0.9965932965278625, "text_region": [[1037.0, 481.0], [1240.0, 481.0], [1240.0, 511.0], [1037.0, 511.0]]}, {"text": "3. Remove SE", "confidence": 0.992504894733429, "text_region": [[418.0, 513.0], [540.0, 513.0], [540.0, 536.0], [418.0, 536.0]]}, {"text": "3. Input Resolution", "confidence": 0.9998311400413513, "text_region": [[719.0, 515.0], [892.0, 518.0], [891.0, 543.0], [718.0, 541.0]]}, {"text": "3. Cosine Learning Rate Decay", "confidence": 0.9998677968978882, "text_region": [[1037.0, 511.0], [1305.0, 513.0], [1305.0, 539.0], [1037.0, 536.0]]}, {"text": "4. Cosine Learning Rate Decay", "confidence": 0.999901294708252, "text_region": [[418.0, 541.0], [684.0, 543.0], [684.0, 568.0], [418.0, 566.0]]}, {"text": " 4. PACT Quantization", "confidence": 0.9242361187934875, "text_region": [[716.0, 543.0], [910.0, 548.0], [910.0, 573.0], [716.0, 568.0]]}, {"text": "4. Feature Map Resolution", "confidence": 0.9997065663337708, "text_region": [[1042.0, 543.0], [1270.0, 543.0], [1270.0, 566.0], [1042.0, 566.0]]}, {"text": "5. Learning Rate Warm-up", "confidence": 0.9698998332023621, "text_region": [[416.0, 566.0], [649.0, 571.0], [649.0, 601.0], [416.0, 596.0]]}, {"text": " 5. Regularization Parameters", "confidence": 0.9909165501594543, "text_region": [[1039.0, 573.0], [1291.0, 573.0], [1291.0, 596.0], [1039.0, 596.0]]}, {"text": "6. FPGM Pruner", "confidence": 0.9953041672706604, "text_region": [[416.0, 596.0], [559.0, 601.0], [558.0, 626.0], [415.0, 621.0]]}, {"text": "6. Learning Rate Warm-up", "confidence": 0.9986522793769836, "text_region": [[1040.0, 596.0], [1273.0, 598.0], [1273.0, 628.0], [1039.0, 626.0]]}, {"text": "7. Light Head", "confidence": 0.9999231100082397, "text_region": [[1042.0, 630.0], [1162.0, 630.0], [1162.0, 653.0], [1042.0, 653.0]]}, {"text": "8. Pre-trained Model", "confidence": 0.9657451510429382, "text_region": [[1039.0, 655.0], [1229.0, 655.0], [1229.0, 685.0], [1039.0, 685.0]]}, {"text": "9. PACT Quantization", "confidence": 0.9950209856033325, "text_region": [[1040.0, 685.0], [1229.0, 688.0], [1229.0, 713.0], [1039.0, 710.0]]}], "img_idx": 0, "score": 0.9659422636032104}
{"type": "figure", "bbox": [154, 887, 810, 1185], "res": [{"text": "Perspective", "confidence": 0.9986883401870728, "text_region": [[185.0, 894.0], [261.0, 894.0], [261.0, 910.0], [185.0, 910.0]]}, {"text": "Scale", "confidence": 0.9998138546943665, "text_region": [[376.0, 889.0], [411.0, 889.0], [411.0, 912.0], [376.0, 912.0]]}, {"text": "Blur", "confidence": 0.9977245926856995, "text_region": [[550.0, 1047.0], [577.0, 1047.0], [577.0, 1068.0], [550.0, 1068.0]]}, {"text": "nouse", "confidence": 0.850503146648407, "text_region": [[517.0, 1116.0], [607.0, 1116.0], [607.0, 1150.0], [517.0, 1150.0]]}], "img_idx": 0, "score": 0.9532002806663513}
{"type": "figure", "bbox": [891, 892, 1542, 1228], "res": [{"text": "7212115024023", "confidence": 0.7510005235671997, "text_region": [[955.0, 1123.0], [1068.0, 1128.0], [1066.0, 1151.0], [953.0, 1145.0]]}], "img_idx": 0, "score": 0.9525312781333923}
