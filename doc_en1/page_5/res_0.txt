{"type": "text", "bbox": [148, 155, 818, 702], "res": [{"text": "some image processing operations to train a text recog-", "confidence": 0.9971258640289307, "text_region": [[143.0, 154.0], [811.0, 156.0], [811.0, 186.0], [143.0, 183.0]]}, {"text": "nizer, such as rotation, perspective distortion, motion blur", "confidence": 0.997165858745575, "text_region": [[148.0, 186.0], [815.0, 186.0], [815.0, 215.0], [148.0, 215.0]]}, {"text": "and Gaussian noise. Those processes are referred to as BDA", "confidence": 0.9927290678024292, "text_region": [[148.0, 215.0], [813.0, 215.0], [813.0, 245.0], [148.0, 245.0]]}, {"text": "(Base Data Augmentation) for short. They are randomly", "confidence": 0.9860718250274658, "text_region": [[150.0, 245.0], [813.0, 245.0], [813.0, 275.0], [150.0, 275.0]]}, {"text": "added to the training images. The experiment shows that", "confidence": 0.9845009446144104, "text_region": [[150.0, 280.0], [813.0, 280.0], [813.0, 302.0], [150.0, 302.0]]}, {"text": "BDA also is useful for the direction classifier training. Be-", "confidence": 0.998731255531311, "text_region": [[150.0, 307.0], [813.0, 307.0], [813.0, 337.0], [150.0, 337.0]]}, {"text": "sides BDA, some new data augmentation operations are", "confidence": 0.9984696507453918, "text_region": [[148.0, 337.0], [815.0, 337.0], [815.0, 367.0], [148.0, 367.0]]}, {"text": "proposed recently for improving the effect of image clas-", "confidence": 0.9855316281318665, "text_region": [[148.0, 367.0], [811.0, 367.0], [811.0, 396.0], [148.0, 396.0]]}, {"text": "sification, for example, AutoAugment (Cubuk et al.2019)", "confidence": 0.9926584362983704, "text_region": [[148.0, 399.0], [808.0, 399.0], [808.0, 429.0], [148.0, 429.0]]}, {"text": "RandAugment (Cubuk et al.2020), CutOut DeVries and", "confidence": 0.9811267852783203, "text_region": [[146.0, 426.0], [815.0, 429.0], [815.0, 458.0], [145.0, 456.0]]}, {"text": "Taylor2017), RandErasing (Zhong et al.2020), HideAnd-", "confidence": 0.9811714291572571, "text_region": [[143.0, 456.0], [811.0, 458.0], [811.0, 490.0], [143.0, 488.0]]}, {"text": "Seek (Singh and Lee[2017), GridMask (Chen 2020), Mixup", "confidence": 0.9812104105949402, "text_region": [[146.0, 486.0], [813.0, 488.0], [813.0, 520.0], [145.0, 518.0]]}, {"text": "(Zhang et al.|2017) and Cutmix (Yun et al.|2019). But the", "confidence": 0.9762628078460693, "text_region": [[150.0, 520.0], [815.0, 520.0], [815.0, 550.0], [150.0, 550.0]]}, {"text": "experiments show that most of them don't work for the", "confidence": 0.9939036965370178, "text_region": [[148.0, 550.0], [813.0, 550.0], [813.0, 580.0], [148.0, 580.0]]}, {"text": "direction classifier training except for RandAugment and", "confidence": 0.9952950477600098, "text_region": [[148.0, 580.0], [815.0, 580.0], [815.0, 610.0], [148.0, 610.0]]}, {"text": "RandErasing. RandAugment works best. Eventually, we add", "confidence": 0.999922513961792, "text_region": [[150.0, 610.0], [813.0, 610.0], [813.0, 639.0], [150.0, 639.0]]}, {"text": "BDA and RandAugment to the training images of the direc-", "confidence": 0.9969848394393921, "text_region": [[148.0, 642.0], [811.0, 642.0], [811.0, 671.0], [148.0, 671.0]]}, {"text": "tion classification.", "confidence": 0.9964451789855957, "text_region": [[148.0, 671.0], [353.0, 671.0], [353.0, 701.0], [148.0, 701.0]]}], "img_idx": 0, "score": 0.9902397990226746}
{"type": "text", "bbox": [149, 1004, 816, 1340], "res": [{"text": "PACT Quantization Quantization allows the neural net", "confidence": 0.9994313716888428, "text_region": [[171.0, 1004.0], [808.0, 1006.0], [808.0, 1038.0], [171.0, 1036.0]]}, {"text": "work model to have lower latency, smaller volume and lower", "confidence": 0.9996497631072998, "text_region": [[148.0, 1038.0], [815.0, 1038.0], [815.0, 1068.0], [148.0, 1068.0]]}, {"text": "computational power consumption. At present, quantiza-", "confidence": 0.9946211576461792, "text_region": [[148.0, 1068.0], [813.0, 1068.0], [813.0, 1098.0], [148.0, 1098.0]]}, {"text": "tion is mainly divided into two categories: offline quanti-", "confidence": 0.9914482235908508, "text_region": [[146.0, 1098.0], [813.0, 1098.0], [813.0, 1128.0], [146.0, 1128.0]]}, {"text": "zation and online quantization. Offline quantization refers", "confidence": 0.9847936630249023, "text_region": [[150.0, 1132.0], [813.0, 1132.0], [813.0, 1155.0], [150.0, 1155.0]]}, {"text": "to a fixed-point quantization method that uses methods such", "confidence": 0.9967250227928162, "text_region": [[148.0, 1160.0], [815.0, 1160.0], [815.0, 1189.0], [148.0, 1189.0]]}, {"text": "as KL divergence and moving average to determine quan-", "confidence": 0.9992067813873291, "text_region": [[143.0, 1187.0], [813.0, 1189.0], [813.0, 1219.0], [143.0, 1217.0]]}, {"text": "tization parameters and does not require retraining. Online", "confidence": 0.9845044016838074, "text_region": [[148.0, 1221.0], [815.0, 1221.0], [815.0, 1251.0], [148.0, 1251.0]]}, {"text": "quantization is to determine quantization parameters dur", "confidence": 0.9911738634109497, "text_region": [[148.0, 1251.0], [808.0, 1251.0], [808.0, 1281.0], [148.0, 1281.0]]}, {"text": "ing the training process, which can provide less quantization", "confidence": 0.9920780658721924, "text_region": [[148.0, 1286.0], [813.0, 1286.0], [813.0, 1309.0], [148.0, 1309.0]]}, {"text": "loss than offline quantization mode.", "confidence": 0.969681441783905, "text_region": [[146.0, 1311.0], [545.0, 1311.0], [545.0, 1341.0], [146.0, 1341.0]]}, {"text": "PACT (PArameterized Clipping acTivation) (Choi et al.", "confidence": 0.9890779256820679, "text_region": [[171.0, 1336.0], [815.0, 1341.0], [815.0, 1373.0], [171.0, 1368.0]]}], "img_idx": 0, "score": 0.9800754189491272}
{"type": "text", "bbox": [150, 700, 816, 1005], "res": [{"text": "tion classification.", "confidence": 0.9964451789855957, "text_region": [[148.0, 671.0], [353.0, 671.0], [353.0, 701.0], [148.0, 701.0]]}, {"text": "Input Resolution In general, when the input resolution", "confidence": 0.9901174902915955, "text_region": [[178.0, 706.0], [813.0, 706.0], [813.0, 729.0], [178.0, 729.0]]}, {"text": "of a normalized image is increased, accuracy will also be", "confidence": 0.9861295819282532, "text_region": [[148.0, 733.0], [815.0, 733.0], [815.0, 763.0], [148.0, 763.0]]}, {"text": "improved. Since the backbone of the direction classifier is", "confidence": 0.9896784424781799, "text_region": [[148.0, 763.0], [815.0, 763.0], [815.0, 793.0], [148.0, 793.0]]}, {"text": "very light, increasing the resolution properly will not lead", "confidence": 0.9993894696235657, "text_region": [[150.0, 798.0], [813.0, 798.0], [813.0, 820.0], [150.0, 820.0]]}, {"text": "to the computation time raise obviously. In the most of the", "confidence": 0.9944050312042236, "text_region": [[148.0, 825.0], [815.0, 825.0], [815.0, 855.0], [148.0, 855.0]]}, {"text": "previous text recognition methods, the height and width of", "confidence": 0.9939830303192139, "text_region": [[148.0, 855.0], [815.0, 855.0], [815.0, 885.0], [148.0, 885.0]]}, {"text": "a normalized image is set as 32 and 100, respectively. How-", "confidence": 0.9966162443161011, "text_region": [[148.0, 887.0], [813.0, 887.0], [813.0, 917.0], [148.0, 917.0]]}, {"text": "ever, in PP-OCR, the height and width is set as 48 and 192,", "confidence": 0.9931436777114868, "text_region": [[148.0, 914.0], [811.0, 914.0], [811.0, 944.0], [148.0, 944.0]]}, {"text": "respectively, to improve the accuracy of the direction classi-", "confidence": 0.9832226037979126, "text_region": [[148.0, 946.0], [813.0, 946.0], [813.0, 976.0], [148.0, 976.0]]}, {"text": "fer.", "confidence": 0.8731980323791504, "text_region": [[150.0, 983.0], [189.0, 983.0], [189.0, 1001.0], [150.0, 1001.0]]}, {"text": "PACT Quantization Quantization allows the neural net", "confidence": 0.9994313716888428, "text_region": [[171.0, 1004.0], [808.0, 1006.0], [808.0, 1038.0], [171.0, 1036.0]]}], "img_idx": 0, "score": 0.9793669581413269}
{"type": "text", "bbox": [888, 1567, 1554, 1781], "res": [{"text": "text recognizer CRNN.", "confidence": 0.9998810887336731, "text_region": [[885.0, 1538.0], [1150.0, 1538.0], [1150.0, 1567.0], [885.0, 1567.0]]}, {"text": "Light Backbone We also adopt MobileNetV3 as the", "confidence": 0.9973960518836975, "text_region": [[912.0, 1568.0], [1552.0, 1568.0], [1552.0, 1597.0], [912.0, 1597.0]]}, {"text": "backbone of the text recognizer which is the same as the", "confidence": 0.9957256317138672, "text_region": [[885.0, 1597.0], [1552.0, 1597.0], [1552.0, 1627.0], [885.0, 1627.0]]}, {"text": "text detection. MobileNetV3_small_x0.5 is selected to bal-", "confidence": 0.9907269477844238, "text_region": [[887.0, 1632.0], [1548.0, 1632.0], [1548.0, 1655.0], [887.0, 1655.0]]}, {"text": "ance accuracy and efficiency empirically. If you're not that", "confidence": 0.9907520413398743, "text_region": [[889.0, 1664.0], [1552.0, 1664.0], [1552.0, 1687.0], [889.0, 1687.0]]}, {"text": "sensitive to the model size, MobileNetV3_small_x1.0 is also", "confidence": 0.9830617904663086, "text_region": [[885.0, 1689.0], [1552.0, 1687.0], [1552.0, 1719.0], [885.0, 1721.0]]}, {"text": "a good choice. The model size is just increased by 2M, the", "confidence": 0.9857941269874573, "text_region": [[885.0, 1721.0], [1552.0, 1721.0], [1552.0, 1751.0], [885.0, 1751.0]]}, {"text": "accuracy is improved obviously", "confidence": 0.9998350143432617, "text_region": [[887.0, 1755.0], [1240.0, 1755.0], [1240.0, 1778.0], [887.0, 1778.0]]}, {"text": "Data Augmentation Besides BDA (Base Data Augmen-", "confidence": 0.995459258556366, "text_region": [[912.0, 1781.0], [1548.0, 1781.0], [1548.0, 1810.0], [912.0, 1810.0]]}], "img_idx": 0, "score": 0.9718180298805237}
{"type": "text", "bbox": [887, 1193, 1553, 1376], "res": [{"text": "parameters to improve the model robustness.", "confidence": 0.9821083545684814, "text_region": [[882.0, 1167.0], [1383.0, 1164.0], [1384.0, 1194.0], [882.0, 1196.0]]}, {"text": "The implementation of the above FPGM Pruner and", "confidence": 0.9833948016166687, "text_region": [[912.0, 1194.0], [1552.0, 1192.0], [1552.0, 1224.0], [912.0, 1226.0]]}, {"text": "PACT quantization is based on PaddleSlim PaddleSlim is", "confidence": 0.987905740737915, "text_region": [[887.0, 1226.0], [1552.0, 1226.0], [1552.0, 1256.0], [887.0, 1256.0]]}, {"text": "a toolkit for model compression. It contains a collection of", "confidence": 0.9792361855506897, "text_region": [[889.0, 1260.0], [1550.0, 1260.0], [1550.0, 1283.0], [889.0, 1283.0]]}, {"text": "compression strategies, such as pruning, fixed point quan-", "confidence": 0.9984713792800903, "text_region": [[887.0, 1288.0], [1550.0, 1288.0], [1550.0, 1318.0], [887.0, 1318.0]]}, {"text": "tization, knowledge distillation, hyperparameter searching", "confidence": 0.987995982170105, "text_region": [[885.0, 1318.0], [1552.0, 1318.0], [1552.0, 1348.0], [885.0, 1348.0]]}, {"text": "neural architecture search.", "confidence": 0.9983523488044739, "text_region": [[887.0, 1350.0], [1178.0, 1350.0], [1178.0, 1373.0], [887.0, 1373.0]]}], "img_idx": 0, "score": 0.9687840938568115}
{"type": "text", "bbox": [887, 749, 1554, 873], "res": [{"text": "Figure 10: Architecture of the text recognizer CRNN. This", "confidence": 0.9928012490272522, "text_region": [[889.0, 754.0], [1548.0, 754.0], [1548.0, 777.0], [889.0, 777.0]]}, {"text": "figure comes from the paper (Shi, Bai, and Yao 2016). The", "confidence": 0.9908308982849121, "text_region": [[887.0, 781.0], [1552.0, 781.0], [1552.0, 811.0], [887.0, 811.0]]}, {"text": "red and gray rectangles show the backbone and head of the", "confidence": 0.999720573425293, "text_region": [[885.0, 811.0], [1554.0, 811.0], [1554.0, 841.0], [885.0, 841.0]]}, {"text": "text recognizer separately.", "confidence": 0.9999590516090393, "text_region": [[885.0, 843.0], [1176.0, 843.0], [1176.0, 873.0], [885.0, 873.0]]}], "img_idx": 0, "score": 0.9661193490028381}
{"type": "text", "bbox": [149, 1712, 814, 1957], "res": [{"text": "The preprocessing of the activation value of the ordinary", "confidence": 0.9997915029525757, "text_region": [[178.0, 1714.0], [813.0, 1714.0], [813.0, 1744.0], [178.0, 1744.0]]}, {"text": "PACT method is based on the ReLU function. All activation", "confidence": 0.9940685033798218, "text_region": [[148.0, 1744.0], [813.0, 1744.0], [813.0, 1774.0], [148.0, 1774.0]]}, {"text": "values greater than a certain threshold are truncated. How-", "confidence": 0.984998881816864, "text_region": [[152.0, 1778.0], [811.0, 1778.0], [811.0, 1801.0], [152.0, 1801.0]]}, {"text": "ever, the activation functions in MobileNetV3 are not only", "confidence": 0.9982457756996155, "text_region": [[150.0, 1806.0], [813.0, 1806.0], [813.0, 1836.0], [150.0, 1836.0]]}, {"text": "ReLU, but also hard swish. Using ordinary PACT quantiza-", "confidence": 0.9835010170936584, "text_region": [[148.0, 1836.0], [808.0, 1836.0], [808.0, 1865.0], [148.0, 1865.0]]}, {"text": "tion leads to a higher quantization loss. Therefore, we mod-", "confidence": 0.9934823513031006, "text_region": [[148.0, 1868.0], [813.0, 1868.0], [813.0, 1898.0], [148.0, 1898.0]]}, {"text": "ify the formula of the activations preprocessing as follows to", "confidence": 0.9956156015396118, "text_region": [[148.0, 1898.0], [815.0, 1898.0], [815.0, 1927.0], [148.0, 1927.0]]}, {"text": " reduce the quantization loss.", "confidence": 0.9933052062988281, "text_region": [[143.0, 1927.0], [464.0, 1927.0], [464.0, 1957.0], [143.0, 1957.0]]}], "img_idx": 0, "score": 0.9630595445632935}
{"type": "text", "bbox": [147, 1339, 817, 1524], "res": [{"text": "loss than offline quantization mode.", "confidence": 0.969681441783905, "text_region": [[146.0, 1311.0], [545.0, 1311.0], [545.0, 1341.0], [146.0, 1341.0]]}, {"text": "PACT (PArameterized Clipping acTivation) (Choi et al.", "confidence": 0.9890779256820679, "text_region": [[171.0, 1336.0], [815.0, 1341.0], [815.0, 1373.0], [171.0, 1368.0]]}, {"text": "2018) is a new online quantification method that removes", "confidence": 0.9996979832649231, "text_region": [[146.0, 1370.0], [813.0, 1370.0], [813.0, 1400.0], [146.0, 1400.0]]}, {"text": "some outliers from the activations in advance. After remov-", "confidence": 0.9895055890083313, "text_region": [[148.0, 1402.0], [813.0, 1402.0], [813.0, 1432.0], [148.0, 1432.0]]}, {"text": "ing the outliers, the model can learn more appropriate quan-", "confidence": 0.9989822506904602, "text_region": [[146.0, 1430.0], [808.0, 1432.0], [808.0, 1464.0], [145.0, 1462.0]]}, {"text": "titative scales. The formula for PACT to preprocess the acti-", "confidence": 0.9995905756950378, "text_region": [[146.0, 1464.0], [811.0, 1464.0], [811.0, 1494.0], [146.0, 1494.0]]}, {"text": "vations is as follows:", "confidence": 0.9806581139564514, "text_region": [[148.0, 1499.0], [383.0, 1499.0], [383.0, 1522.0], [148.0, 1522.0]]}], "img_idx": 0, "score": 0.9589827060699463}
{"type": "text", "bbox": [887, 1779, 1552, 1903], "res": [{"text": "Data Augmentation Besides BDA (Base Data Augmen-", "confidence": 0.995459258556366, "text_region": [[912.0, 1781.0], [1548.0, 1781.0], [1548.0, 1810.0], [912.0, 1810.0]]}, {"text": "tation) which is often used in text recognition as mentioned", "confidence": 0.9891489148139954, "text_region": [[885.0, 1813.0], [1552.0, 1813.0], [1552.0, 1842.0], [885.0, 1842.0]]}, {"text": "earlier, TIA (Luo et al. 2020) also is an effective data aug-", "confidence": 0.9948781132698059, "text_region": [[887.0, 1842.0], [1548.0, 1842.0], [1548.0, 1872.0], [887.0, 1872.0]]}, {"text": "mentation method for text recognition. As shown in Figure", "confidence": 0.9917116165161133, "text_region": [[885.0, 1875.0], [1554.0, 1875.0], [1554.0, 1904.0], [885.0, 1904.0]]}], "img_idx": 0, "score": 0.9536663293838501}
{"type": "text", "bbox": [887, 1444, 1553, 1567], "res": [{"text": "In this section, the details of nine strategies for enhancing the", "confidence": 0.9955481886863708, "text_region": [[885.0, 1446.0], [1552.0, 1446.0], [1552.0, 1476.0], [885.0, 1476.0]]}, {"text": "model ability or reducing the model size of a text recognizer", "confidence": 0.9925819039344788, "text_region": [[885.0, 1476.0], [1552.0, 1476.0], [1552.0, 1506.0], [885.0, 1506.0]]}, {"text": "will be introduced. Figure10shows the architecture of the", "confidence": 0.996536135673523, "text_region": [[887.0, 1508.0], [1552.0, 1508.0], [1552.0, 1538.0], [887.0, 1538.0]]}, {"text": "text recognizer CRNN.", "confidence": 0.9998810887336731, "text_region": [[885.0, 1538.0], [1150.0, 1538.0], [1150.0, 1567.0], [885.0, 1567.0]]}], "img_idx": 0, "score": 0.9527912139892578}
{"type": "text", "bbox": [886, 1071, 1554, 1194], "res": [{"text": "We used the improved PACT quantification method to", "confidence": 0.9994683861732483, "text_region": [[912.0, 1072.0], [1554.0, 1072.0], [1554.0, 1102.0], [912.0, 1102.0]]}, {"text": "quantify the direction classifier model. In addition, L2 reg.", "confidence": 0.9927420020103455, "text_region": [[887.0, 1105.0], [1545.0, 1105.0], [1545.0, 1134.0], [887.0, 1134.0]]}, {"text": "ularization with a coefficient of O.001 is added to the PACT", "confidence": 0.9902350306510925, "text_region": [[887.0, 1134.0], [1550.0, 1134.0], [1550.0, 1164.0], [887.0, 1164.0]]}, {"text": "parameters to improve the model robustness.", "confidence": 0.9821083545684814, "text_region": [[882.0, 1167.0], [1383.0, 1164.0], [1384.0, 1194.0], [882.0, 1196.0]]}, {"text": "The implementation of the above FPGM Pruner and", "confidence": 0.9833948016166687, "text_region": [[912.0, 1194.0], [1552.0, 1192.0], [1552.0, 1224.0], [912.0, 1226.0]]}], "img_idx": 0, "score": 0.9520726799964905}
{"type": "title", "bbox": [886, 1401, 1174, 1437], "res": [{"text": "2.3Text Recognition", "confidence": 0.9889622926712036, "text_region": [[882.0, 1402.0], [1176.0, 1405.0], [1176.0, 1437.0], [882.0, 1435.0]]}], "img_idx": 0, "score": 0.843676745891571}
{"type": "figure", "bbox": [894, 153, 1541, 724], "res": [{"text": "\"state\"", "confidence": 0.9950224161148071, "text_region": [[1104.0, 158.0], [1171.0, 158.0], [1171.0, 181.0], [1104.0, 181.0]]}, {"text": "Predicted", "confidence": 0.9995551705360413, "text_region": [[1280.0, 149.0], [1347.0, 149.0], [1347.0, 174.0], [1280.0, 174.0]]}, {"text": "Transcription", "confidence": 0.9701630473136902, "text_region": [[917.0, 179.0], [1012.0, 179.0], [1012.0, 204.0], [917.0, 204.0]]}, {"text": "sequence", "confidence": 0.9992349147796631, "text_region": [[1280.0, 172.0], [1342.0, 172.0], [1342.0, 195.0], [1280.0, 195.0]]}, {"text": "Layer", "confidence": 0.9997330904006958, "text_region": [[965.0, 202.0], [1012.0, 202.0], [1012.0, 227.0], [965.0, 227.0]]}, {"text": "-s-t-aatte", "confidence": 0.9728676676750183, "text_region": [[1056.0, 211.0], [1220.0, 211.0], [1220.0, 236.0], [1056.0, 236.0]]}, {"text": "Per-frame", "confidence": 0.9988266825675964, "text_region": [[1280.0, 202.0], [1349.0, 202.0], [1349.0, 227.0], [1280.0, 227.0]]}, {"text": "predictions", "confidence": 0.9932368993759155, "text_region": [[1282.0, 225.0], [1351.0, 225.0], [1351.0, 241.0], [1282.0, 241.0]]}, {"text": "(disbritutions)", "confidence": 0.9971454739570618, "text_region": [[1280.0, 241.0], [1372.0, 241.0], [1372.0, 264.0], [1280.0, 264.0]]}, {"text": "Deep", "confidence": 0.9969969987869263, "text_region": [[1282.0, 291.0], [1324.0, 291.0], [1324.0, 316.0], [1282.0, 316.0]]}, {"text": "bidirectional", "confidence": 0.9816051721572876, "text_region": [[1282.0, 309.0], [1360.0, 309.0], [1360.0, 332.0], [1282.0, 332.0]]}, {"text": "Recurrent", "confidence": 0.9602330327033997, "text_region": [[939.0, 336.0], [1012.0, 342.0], [1010.0, 367.0], [937.0, 361.0]]}, {"text": "LSTM", "confidence": 0.9982969164848328, "text_region": [[1282.0, 330.0], [1330.0, 330.0], [1330.0, 355.0], [1282.0, 355.0]]}, {"text": " Head", "confidence": 0.9363398551940918, "text_region": [[1397.0, 332.0], [1485.0, 332.0], [1485.0, 367.0], [1397.0, 367.0]]}, {"text": "Layers", "confidence": 0.9988651871681213, "text_region": [[956.0, 367.0], [1014.0, 367.0], [1014.0, 387.0], [956.0, 387.0]]}, {"text": "Feature", "confidence": 0.9990507364273071, "text_region": [[1282.0, 417.0], [1335.0, 417.0], [1335.0, 442.0], [1282.0, 442.0]]}, {"text": "sequence", "confidence": 0.9519915580749512, "text_region": [[1284.0, 442.0], [1337.0, 442.0], [1337.0, 458.0], [1284.0, 458.0]]}, {"text": "Convolutional", "confidence": 0.9997037649154663, "text_region": [[1279.0, 482.0], [1371.0, 476.0], [1373.0, 501.0], [1281.0, 507.0]]}, {"text": "Convolutional", "confidence": 0.9983693361282349, "text_region": [[917.0, 548.0], [1009.0, 548.0], [1009.0, 571.0], [917.0, 571.0]]}, {"text": "Layers", "confidence": 0.9996840357780457, "text_region": [[963.0, 573.0], [1009.0, 573.0], [1009.0, 591.0], [963.0, 591.0]]}, {"text": "Backbone", "confidence": 0.9988107681274414, "text_region": [[1383.0, 570.0], [1546.0, 578.0], [1544.0, 617.0], [1381.0, 609.0]]}, {"text": "Convolutional", "confidence": 0.9971408843994141, "text_region": [[1287.0, 587.0], [1370.0, 587.0], [1370.0, 603.0], [1287.0, 603.0]]}, {"text": "feature maps", "confidence": 0.97836834192276, "text_region": [[1284.0, 607.0], [1360.0, 607.0], [1360.0, 623.0], [1284.0, 623.0]]}, {"text": "Input image", "confidence": 0.9982441067695618, "text_region": [[1282.0, 674.0], [1363.0, 674.0], [1363.0, 699.0], [1282.0, 699.0]]}], "img_idx": 0, "score": 0.9688653349876404}
